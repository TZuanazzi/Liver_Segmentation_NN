{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TZuanazzi/Liver_Segmentation_NN/blob/main/gerador_de_imagem_e_ataque.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etcH3p7d4Sg3"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.kill(os.getpid(), 9)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VZcuK6wylQ9"
      },
      "outputs": [],
      "source": [
        "# !pip install  torchmetrics\n",
        "# !pip install  tqdm\n",
        "# !pip install  torch\n",
        "# !pip install  torchvision\n",
        "# !pip install  TorchAudio\n",
        "# !pip install  Cython\n",
        "# !pip install  torchattacks\n",
        "# !pip install  opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxdrEWqZs38r"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "class DresdenDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        basename = os.path.basename(image_dir)\n",
        "        self.image_names = [filename for filename in os.listdir(image_dir) if filename.startswith(\"image\")]\n",
        "        self.label_names = [filename for filename in os.listdir(image_dir) if filename.startswith(\"mask\")]\n",
        "\n",
        "        # Sort the image and label names based on the numeric part extracted from filenames\n",
        "        self.image_names.sort(key=lambda x: int(x[5:7]))  # Extract the two-digit number from \"imageXX.png\"\n",
        "        self.label_names.sort(key=lambda x: int(x[4:6]))  # Extract the two-digit number from \"maskXX.png\"\n",
        "\n",
        "        print(\"image_names:\", self.image_names)\n",
        "        print(\"label:\", self.label_names)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def classes(self):\n",
        "        return torch.Tensor([0,1])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        image = np.array(Image.open(os.path.join(self.image_dir, self.image_names[idx])).convert('RGB'))\n",
        "        label1 = np.array(Image.open(os.path.join(self.image_dir, self.label_names[idx])).convert('RGB'))\n",
        "        # to use just three conditions, we create another label with np.zeros\n",
        "        label = np.zeros(np.shape(label1), np.uint8)[:,:,0:2]\n",
        "        label[:,:,0][label1[:,:,0]>125] = 1\n",
        "        label[:,:,1][label1[:,:,0]<125] = 1\n",
        "\n",
        "        dictionary = {'image0': image, 'image1': label}\n",
        "\n",
        "        if self.transform is not None:\n",
        "            dictionary = self.transform(dictionary)\n",
        "\n",
        "        return dictionary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpEZiDeMs-Bu",
        "outputId": "b988ff7f-2bcf-4550-b905-1d2c11cd192e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 576, 576])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# import time\n",
        "\n",
        "# Since nn.Sequential does not handle multiple inputs, create mySequential to\n",
        "# handle it, inhiriting from nn.Sequential\n",
        "class mySequential(nn.Sequential):\n",
        "    def forward(self, *inputs):\n",
        "        for module in self._modules.values():\n",
        "            if type(inputs) == tuple:\n",
        "                inputs = module(*inputs)\n",
        "            else:\n",
        "                inputs = module(inputs)\n",
        "        return inputs\n",
        "\n",
        "\n",
        "class block_standard(nn.Module):\n",
        "    #defining block expansion\n",
        "    expansion: int = 1\n",
        "    # To devide the 'out_channels' by 2 in the standard, we create this variab.\n",
        "    out_multiply: int = 2\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, identity_downsample=None, stride=1, up=False):\n",
        "\n",
        "        super(block_standard, self).__init__()\n",
        "\n",
        "        self.up = up\n",
        "        self.stride = stride\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        if self.up:\n",
        "            self.conv1 = nn.ConvTranspose2d(in_channels, out_channels,\n",
        "                                            kernel_size=stride, stride=stride)\n",
        "        else:\n",
        "            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
        "                                   stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                               padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.identity_downsample = identity_downsample\n",
        "\n",
        "    def forward(self, x, long_skip=None):\n",
        "        identity = x\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        if (self.identity_downsample is not None):\n",
        "            identity = self.identity_downsample(identity)\n",
        "        # if long_skip==None: print('long skip none')\n",
        "        if long_skip is not None:\n",
        "            x = torch.cat((x, long_skip), dim=1)\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "\n",
        "        del identity\n",
        "\n",
        "        return x, long_skip\n",
        "\n",
        "\n",
        "class block_bottleneck(nn.Module):\n",
        "    # defining block expansion\n",
        "    expansion: int = 4\n",
        "    # To devide the 'out_channels' by 2 in the standard, we create this variab.\n",
        "    out_multiply: int = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, identity_scale=None, stride=1, up=False):\n",
        "\n",
        "        super(block_bottleneck, self).__init__()\n",
        "\n",
        "        self.up = up\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        if self.up:\n",
        "            self.expansion = 2\n",
        "        else:\n",
        "            self.expansion = 4\n",
        "\n",
        "        if self.up:\n",
        "            self.conv1 = nn.ConvTranspose2d(in_channels, out_channels,\n",
        "                                            kernel_size=stride, stride=stride)\n",
        "        else:\n",
        "            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
        "                                   stride=1, padding=0)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        if self.up:\n",
        "            self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                                   stride=1, padding=1)\n",
        "        else:\n",
        "            self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                                   stride=stride, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels*self.expansion,\n",
        "                               kernel_size=1, stride=1, padding=0)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels*self.expansion)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.identity_scale = identity_scale\n",
        "\n",
        "    def forward(self, x, long_skip=None):\n",
        "        identity = x\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "\n",
        "        if self.identity_scale is not None:\n",
        "            identity = self.identity_scale(identity)\n",
        "        # if long_skip==None: print('long skip none')\n",
        "        if long_skip is not None:\n",
        "            x = torch.cat((x, long_skip), dim=1)\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "\n",
        "        del identity\n",
        "\n",
        "        return x, long_skip\n",
        "\n",
        "\n",
        "class UResNet(nn.Module): # [3, 4, 6, 3]\n",
        "\n",
        "    def __init__(self, block, layers, image_channels, num_classes):\n",
        "\n",
        "        super(UResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        # First Convolutions\n",
        "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7,\n",
        "                               stride=2, padding=3)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.long_skip = []\n",
        "\n",
        "        # ResNet Layers\n",
        "        self.layer1 = self._make_layer(block, layers[0], out_channels=64, stride=1)\n",
        "        self.layer2 = self._make_layer(block, layers[1], out_channels=128, stride=2)\n",
        "        self.layer3 = self._make_layer(block, layers[2], out_channels=256, stride=2)\n",
        "        self.layer4 = self._make_layer(block, layers[3], out_channels=512, stride=2)\n",
        "\n",
        "        # ResNet Layers\n",
        "        self.layer5 = self._make_layer(block, layers[3], out_channels=512, stride=2, up=True)\n",
        "        self.layer6 = self._make_layer(block, layers[2], out_channels=256, stride=2, up=True)\n",
        "        self.layer7 = self._make_layer(block, layers[1], out_channels=128, stride=2, up=True)\n",
        "        self.layer8 = self._make_layer(block, layers[0], out_channels=64, stride=1)\n",
        "\n",
        "        # Last Convolutions\n",
        "        self.conv_last1 = nn.ConvTranspose2d(self.in_channels, 64,\n",
        "                                             kernel_size=2, stride=2, padding=0)\n",
        "        self.conv_last2 = nn.ConvTranspose2d(64*2, num_classes,\n",
        "                                             kernel_size=2, stride=2, padding=0)\n",
        "        self.bn2 = nn.BatchNorm2d(num_classes)\n",
        "        self.Softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        self.long_skip = [0, 0, 0, 0]\n",
        "        self.long_skip[0] = x\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x, temp = self.layer1(x, None)\n",
        "        self.long_skip[1] = x\n",
        "        x, temp = self.layer2(x, None)\n",
        "        self.long_skip[2] = x\n",
        "        x, temp = self.layer3(x, None)\n",
        "        self.long_skip[3] = x\n",
        "        x, temp = self.layer4(x, None)\n",
        "\n",
        "        self.long_skip = self.long_skip[::-1]\n",
        "\n",
        "        x, temp = self.layer5(x, self.long_skip[0])\n",
        "        x, temp = self.layer6(x, self.long_skip[1])\n",
        "        x, temp = self.layer7(x, self.long_skip[2])\n",
        "        x, temp = self.layer8(x, None)\n",
        "        x = self.conv_last1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = torch.cat((x, self.long_skip[3]), dim=1)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv_last2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.Softmax(x)\n",
        "\n",
        "        del self.long_skip, temp\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "    def _make_layer(self, block, num_residual_blocks,\n",
        "                         out_channels, stride, up=False):\n",
        "        identity_scale = None\n",
        "        layers = []\n",
        "\n",
        "        if up==False:\n",
        "            if stride != 1 or self.in_channels != out_channels*block.expansion:\n",
        "                identity_scale = mySequential(nn.Conv2d(self.in_channels,\n",
        "                                                        out_channels*block.expansion,\n",
        "                                                        kernel_size=1,\n",
        "                                                        stride=stride),\n",
        "                                              nn.BatchNorm2d(out_channels*block.expansion))\n",
        "\n",
        "            layers.append(block(self.in_channels, out_channels,\n",
        "                                identity_scale, stride))\n",
        "            self.in_channels = out_channels*block.expansion\n",
        "\n",
        "            for i in range(num_residual_blocks-1):\n",
        "                layers.append(block(self.in_channels, out_channels))\n",
        "\n",
        "        else:\n",
        "            if stride != 1 or self.in_channels != out_channels*block.expansion:\n",
        "                identity_scale = mySequential(nn.ConvTranspose2d(self.in_channels,\n",
        "                                                                 out_channels*block.expansion,\n",
        "                                                                 kernel_size=stride,\n",
        "                                                                 stride=stride),\n",
        "                                              nn.BatchNorm2d(out_channels*block.expansion))\n",
        "            # This is to devide the 'out_channels' by 2 in the standard block\n",
        "            # if you see, the output channels is really half of the value in\n",
        "            # this block.\n",
        "            out_channels = int(out_channels/block.out_multiply)\n",
        "            layers.append(block(self.in_channels, out_channels,\n",
        "                                identity_scale, stride, up=True))\n",
        "\n",
        "            if stride==1 and up==True:\n",
        "                self.in_channels = out_channels*2\n",
        "            elif block.expansion == 1 and up==True:\n",
        "                self.in_channels = out_channels*2\n",
        "            else:\n",
        "                self.in_channels = out_channels*4\n",
        "\n",
        "            for i in range(num_residual_blocks-1):\n",
        "                layers.append(block(self.in_channels, out_channels, up=True))\n",
        "\n",
        "        return mySequential(*layers)\n",
        "\n",
        "def UResNet18(in_channels=3, num_classes=3):\n",
        "    return UResNet(block_standard, [2, 2, 2, 2], in_channels, num_classes)\n",
        "\n",
        "def UResNet34(in_channels=3, num_classes=3):\n",
        "    return UResNet(block_standard, [3, 4, 6, 3], in_channels, num_classes)\n",
        "\n",
        "def UResNet50(in_channels=3, num_classes=3):\n",
        "    return UResNet(block_bottleneck, [3, 4, 6, 3], in_channels, num_classes)\n",
        "\n",
        "def UResNet101(in_channels=3, num_classes=3):\n",
        "    return UResNet(block_bottleneck, [3, 4, 23, 3], in_channels, num_classes)\n",
        "\n",
        "def UResNet152(in_channels=3, num_classes=3):\n",
        "    return UResNet(block_bottleneck, [3, 8, 36, 3], in_channels, num_classes)\n",
        "\n",
        "\n",
        "def test():\n",
        "    net = UResNet34()\n",
        "    # working sizes 224, 256, 288\n",
        "    x = torch.randn(2, 3, 575, 575)\n",
        "    if torch.cuda.is_available():\n",
        "        y = net(x).to('cuda')\n",
        "    else:\n",
        "        y = net(x)\n",
        "    print(y.shape)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    test()\n",
        "\n",
        "# print('- Time taken:', time.time()-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-rDaXRts-Ap"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This file is used together with the 'train.py' file to help in the training and\n",
        "testing process with util functions.\n",
        "'''\n",
        "import torch\n",
        "# from dataset import DresdenDataset\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision.transforms.functional as tf\n",
        "from torchvision.transforms import Compose\n",
        "from torchvision.utils import save_image\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from torchmetrics import Accuracy\n",
        "from torchmetrics import F1Score\n",
        "from torchmetrics import JaccardIndex\n",
        "from torchmetrics import Recall\n",
        "from torchmetrics import Precision\n",
        "from torchmetrics import Specificity\n",
        "\n",
        "\n",
        "\n",
        "# The next functions are functional transforms, used to apply functions in a way\n",
        "# controled by the user. So we can apply, for example, in the data image and in\n",
        "# the label image (so it is called deterministic, because we can determine the\n",
        "# same transformation to be applied in more then one image). This is the unique\n",
        "# way to apply the same transformation to more then one different image in torch\n",
        "\n",
        "class ToTensor(object):\n",
        "    '''Function to transform a ndarray in a tensor\n",
        "\n",
        "    n: int (input)\n",
        "        number of non-mask images to convert to tensor (the rest will be\n",
        "        converted without scaling to [0.0,1.0])'''\n",
        "    def __init__(self, n):\n",
        "        self.n = n\n",
        "\n",
        "    def __call__(self, images):\n",
        "        for i, image in enumerate(images):\n",
        "            if i < self.n:\n",
        "                images[image] = tf.to_tensor(images[image])\n",
        "            else:\n",
        "                images[image] = torch.from_numpy(images[image])\n",
        "                images[image] = torch.permute(images[image], (2,0,1))\n",
        "\n",
        "        return images\n",
        "\n",
        "\n",
        "class Rotate(object):\n",
        "    '''Function to rotate an image, the input is a dictionary\n",
        "\n",
        "    images: 'dictionary' (input)\n",
        "        dictionary with images;\n",
        "    limit: 'list'\n",
        "        a list 'int' with smaller and larger angles to rotate (e.g. [0, 90]);\n",
        "    p: 'float'\n",
        "        probability to rotate;\n",
        "\n",
        "    dictionary: 'dictionary' (output)\n",
        "        dictionary with cropped images with keys 'image0', 'image1', etc.\n",
        "    '''\n",
        "    def __init__(self,**kwargs):\n",
        "        # 'limit' is a 'list' that defines the lower and upper angular limits\n",
        "        limit = kwargs.get('limit')\n",
        "        if not limit: limit = [0, 360]\n",
        "        self.limit = limit\n",
        "        # 'p' is 'float' the probability to happen a rotate\n",
        "        p = kwargs.get('p')\n",
        "        if not p: p = 0.5\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, images):\n",
        "        if random.random() > 1-self.p:\n",
        "            angle = random.randint(self.limit[0], self.limit[1])\n",
        "            for i, image in enumerate(images):\n",
        "                images[image] = tf.rotate(images[image], angle)\n",
        "\n",
        "        return images\n",
        "\n",
        "\n",
        "class CenterCrop(object):\n",
        "    '''Function to center crop one or multiple images\n",
        "\n",
        "    size: 'list' (input)\n",
        "        input list with size (e.g. '[400,200]');\n",
        "    images: 'dictionary' (input) (output)\n",
        "        dictionary with images.\n",
        "    '''\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "\n",
        "    def __call__(self, images):\n",
        "        for image in images:\n",
        "            images[image] = tf.center_crop(images[image], self.size)\n",
        "\n",
        "        return images\n",
        "\n",
        "\n",
        "class Resize(object):\n",
        "    '''Function to resize one or multiple images\n",
        "\n",
        "    size: 'list' (input)\n",
        "        input list with size (e.g. '[400,200]');\n",
        "    images: 'dictionary' (input) (output)\n",
        "        dictionary with images.\n",
        "    '''\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    def __call__(self, images):\n",
        "        for image in images:\n",
        "            images[image] = tf.resize(images[image], self.size)\n",
        "\n",
        "        return images\n",
        "\n",
        "\n",
        "class FlipHorizontal(object):\n",
        "    '''Horizontally flip images randomly\n",
        "\n",
        "    p: 'float' (input)\n",
        "        probability to flip (from 0.0 to 1.0).\n",
        "    '''\n",
        "    def __init__(self, p):\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, images):\n",
        "        if random.random() > 1-self.p:\n",
        "            for image in images:\n",
        "                images[image] = tf.hflip(images[image])\n",
        "\n",
        "        return images\n",
        "\n",
        "\n",
        "class FlipVertical(object):\n",
        "    '''Vertically flip images randomly\n",
        "\n",
        "    p: 'float' (input)\n",
        "        probability to flip (from 0.0 to 1.0).\n",
        "    '''\n",
        "    def __init__(self, p):\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, images):\n",
        "        if random.random() > 1-self.p:\n",
        "            for image in images:\n",
        "                images[image] = tf.vflip(images[image])\n",
        "\n",
        "        return images\n",
        "\n",
        "\n",
        "class Normalize(object):\n",
        "    '''Normalizing 'n' images of a given set of images\n",
        "\n",
        "    n: int (input)\n",
        "        number of images to normalize;\n",
        "    mean: list (input)\n",
        "        mean to normalize;\n",
        "    std: list (input)\n",
        "        stadard deviation to normalize.\n",
        "    '''\n",
        "    def __init__(self, n=1, mean=0.5, std=0.5):\n",
        "        self.n = n\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, images):\n",
        "        for i, image in enumerate(images):\n",
        "            if i < self.n:\n",
        "                images[image] = tf.normalize(images[image], self.mean, self.std)\n",
        "\n",
        "        return images\n",
        "\n",
        "\n",
        "class Affine(object):\n",
        "    '''Affining images\n",
        "\n",
        "    size: list (input)\n",
        "        maximum higher and width to translate image (normally the image size);\n",
        "    scale: float (input)\n",
        "        scale to perform affine (between 0 and 1.0);\n",
        "    p: float (input)\n",
        "        probability to thange.'''\n",
        "    def __init__(self, size=[0,0], scale=0.5, p=0.5):\n",
        "        self.size = size\n",
        "        self.scale = scale\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, images):\n",
        "        if random.random() > 1-self.p:\n",
        "            angle = random.random()*self.scale*360\n",
        "            shear = random.random()*self.scale*360\n",
        "            translate = [i*random.random()*self.scale for i in self.size]\n",
        "            for image in images:\n",
        "                images[image] = tf.affine(images[image], angle=angle,\n",
        "                                         translate=translate, scale=1-self.scale,\n",
        "                                         shear=shear)\n",
        "        return images\n",
        "\n",
        "\n",
        "#%% Saving and Loading Checkpoints, getting the Data Loaders\n",
        "\n",
        "# saving checkpoints\n",
        "def save_checkpoint(state, filename='my_checkpoint.pth.tar'):\n",
        "    print('\\n- Saving Checkpoint...')\n",
        "    torch.save(state, filename)\n",
        "\n",
        "# loading checkpoints\n",
        "def load_checkpoint(checkpoint, model, optimizer=None):\n",
        "    print('\\n- Loading Checkpoint...')\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    if optimizer:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "\n",
        "# getting loaders given directories and other informations\n",
        "def get_loaders(train_image_dir,\n",
        "                valid_percent,\n",
        "                test_percent,\n",
        "                batch_size,\n",
        "                image_height,\n",
        "                image_width,\n",
        "                num_workers=1,\n",
        "                pin_memory=True,\n",
        "                val_image_dir=None,\n",
        "                clip_valid=1.0,\n",
        "                clip_train=1.0,\n",
        "                clip_test=1.0):\n",
        "\n",
        "    # first, defining transformations to be applied in the train images to be loaded\n",
        "    transform_train_0 = Compose([ToTensor(n=1),\n",
        "                                 Resize(size=[image_height, image_width]),\n",
        "                                 # mean and std, obtained from Dresden Dataset\n",
        "                                 # for segmentation\n",
        "                                 Normalize(n=1, mean=[0.4338, 0.31936, 0.312387],\n",
        "                                           std=[0.1904, 0.15638, 0.15657])]\n",
        "                                )\n",
        "    # defining the same, but for validation and testing images (can be different)\n",
        "    transform_valid_0 = Compose([ToTensor(n=1),\n",
        "                                 Resize(size=[image_height, image_width]),\n",
        "                                 # defining again if validation dataset is dif.\n",
        "                                 Normalize(n=1, mean=[0.4338, 0.31936, 0.312387],\n",
        "                                           std=[0.1904, 0.15638, 0.15657])]\n",
        "                                )\n",
        "\n",
        "    # second, defining the number of transformations per directory in\n",
        "    # 'train_image_dir' defines the data augmantation (1 for no augmentation\n",
        "    # and 5 for 5 times augmentation)\n",
        "    transformations_per_dataset = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "                                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "    # transformations_per_dataset = [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
        "    #                                5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
        "    # transformations_per_dataset = [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
        "    #                                 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
        "    #                                 10, 10, 10, 10, 10, 10, 10, 10]\n",
        "\n",
        "    # third, reading the dataset in a as a 'torch.utils.data.Dataset' instance.\n",
        "    # it is only for images in 'train_image_dir[0]', further we will accounts\n",
        "    # for the rest of the directories\n",
        "    train_dataset = DresdenDataset(image_dir=train_image_dir[0],\n",
        "                                  transform=transform_train_0)\n",
        "\n",
        "    # concatenate the other directories in 'train_image_dir[:]' in a larger\n",
        "    # 'torhc.utils.data.Dataset'. after we will concatenate more for augmentat.\n",
        "    for n in range(1, len(train_image_dir)):\n",
        "        dataset_train_temp = DresdenDataset(image_dir=train_image_dir[n],\n",
        "                                           transform=transform_train_0)\n",
        "        # to use 'train_dataset' here in right, we have to define it before\n",
        "        train_dataset = torch.utils.data.ConcatDataset([train_dataset,\n",
        "                                                        dataset_train_temp])\n",
        "\n",
        "    # using part of the training data as test dataset\n",
        "    test_dataset_size = int(test_percent*len(train_dataset))\n",
        "    rest_size = int((1-test_percent)*len(train_dataset))\n",
        "    if test_dataset_size+rest_size != len(train_dataset):\n",
        "        rest_size += 1\n",
        "    (test_dataset, _) = random_split(train_dataset, [test_dataset_size, rest_size],\n",
        "                                     generator=(torch.Generator().manual_seed(40)))\n",
        "\n",
        "    # defining the validation dataset, using part of the 'train_dataset', or\n",
        "    # using a specific dataset for validation, if 'val_image_dir' is not 'None'\n",
        "    if not val_image_dir:\n",
        "        valid_dataset_size = int(valid_percent*len(train_dataset))\n",
        "        train_dataset_size = int((1-valid_percent)*len(train_dataset))\n",
        "        # adding one to train_dataset_size if 'int' operation removed it\n",
        "        if valid_dataset_size+train_dataset_size != len(train_dataset):\n",
        "            train_dataset_size += 1\n",
        "        (train_dataset, valid_dataset) = random_split(train_dataset,\n",
        "                                         [train_dataset_size, valid_dataset_size],\n",
        "                                         generator=torch.Generator().manual_seed(20))\n",
        "    else:\n",
        "        valid_dataset = DresdenDataset(image_dir=val_image_dir[0],\n",
        "                                      transform=transform_valid_0)\n",
        "        for n in range(1, len(val_image_dir)):\n",
        "            dataset_val_temp = DresdenDataset(image_dir=val_image_dir[n],\n",
        "                                             transform=transform_valid_0)\n",
        "            valid_dataset = torch.utils.data.ConcatDataset([valid_dataset,\n",
        "                                                            dataset_val_temp])\n",
        "\n",
        "    # concatenating the augmented data, in case 'transf..._per_dataset' > 1\n",
        "    for n in range(0,len(train_image_dir)):\n",
        "        for m in range(1, transformations_per_dataset[n]):\n",
        "            # first we specify the transformation (depending on the 'm' value)\n",
        "            if m < 2:\n",
        "                transformation = Compose([ToTensor(n=1),\n",
        "                                          Resize(size=[image_height, image_width]),\n",
        "                                          # Mean and std, obtained from the dataset\n",
        "                                          Normalize(n=1, mean=[0.4338, 0.31936, 0.312387],\n",
        "                                                    std=[0.1904, 0.15638, 0.15657])]\n",
        "                                          )\n",
        "            else:\n",
        "                transformation = Compose([ToTensor(n=1),\n",
        "                                          Resize(size=[image_height, image_width]),\n",
        "                                          # Mean and std, obtained from the dataset\n",
        "                                          Normalize(n=1, mean=[0.4338, 0.31936, 0.312387],\n",
        "                                                    std=[0.1904, 0.15638, 0.15657])]\n",
        "                                          )\n",
        "            # then we apply this transformation to read the dataset as 'torch.utils.data.Dataset'\n",
        "            dataset_train_temp = DresdenDataset(image_dir=train_image_dir[n],\n",
        "                                               transform=transformation)\n",
        "            train_dataset = torch.utils.data.ConcatDataset([train_dataset, dataset_train_temp])\n",
        "\n",
        "    # splitting the dataset, to deminish if 'clip_valid'<1 for fast testing\n",
        "    if clip_train < 1:\n",
        "        print('\\n- Splitting Training Dataset ',clip_train*100,'%')\n",
        "        train_mini = int(clip_train*len(train_dataset))\n",
        "        temp_mini = int((1-clip_train)*len(train_dataset))\n",
        "        if train_mini+temp_mini != len(train_dataset):\n",
        "            temp_mini += 1\n",
        "        (train_dataset, _) = random_split(train_dataset,[train_mini, temp_mini],\n",
        "                                          generator=torch.Generator().manual_seed(40))\n",
        "    if clip_valid < 1:\n",
        "        print('\\n- Splitting Validation Dataset ',clip_valid*100,'%')\n",
        "        valid_mini = int(clip_valid*len(valid_dataset))\n",
        "        temp_mini = int((1-clip_valid)*len(valid_dataset))\n",
        "        if valid_mini+temp_mini != len(valid_dataset):\n",
        "            temp_mini += 1\n",
        "        (valid_dataset, _) = random_split(valid_dataset,[valid_mini, temp_mini],\n",
        "                                          generator=torch.Generator().manual_seed(30))\n",
        "\n",
        "    if clip_test < 1:\n",
        "        print('\\n- Splitting Testing Dataset ',clip_test*100,'%')\n",
        "        test_mini = int(clip_test*len(test_dataset))\n",
        "        temp_mini = int((1-clip_test)*len(test_dataset))\n",
        "        if test_mini+temp_mini != len(test_dataset):\n",
        "            temp_mini += 1\n",
        "        (test_dataset, _) = random_split(test_dataset, [test_mini, temp_mini],\n",
        "                                         generator=torch.Generator().manual_seed(50))\n",
        "\n",
        "    # obtaining dataloader from the datasets defined above\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                              num_workers=num_workers,\n",
        "                              pin_memory=pin_memory, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
        "                             num_workers=num_workers,\n",
        "                             pin_memory=pin_memory)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size,\n",
        "                              num_workers=num_workers,\n",
        "                              pin_memory=pin_memory)\n",
        "\n",
        "    return train_loader, test_loader, valid_loader\n",
        "\n",
        "\n",
        "#%% Function to check accuracy\n",
        "\n",
        "def check_accuracy(loader, model, loss_fn, output_labels,\n",
        "                   device='cuda' if torch.cuda.is_available() else 'cpu',\n",
        "                   **kwargs):\n",
        "    # Here 'output_labels == 2' means binary. We use this conditional because\n",
        "    # the model we are using returns a multilabel data, when we use this model\n",
        "    # with 2 labels, it becomes binary.\n",
        "    if output_labels == 2:\n",
        "        accuracy = Accuracy(task='binary', num_labels=output_labels).to(device)\n",
        "        f1score = F1Score(task='binary', num_labels=output_labels).to(device)\n",
        "        intersection = JaccardIndex(task='binary', num_labels=output_labels).to(device)\n",
        "        recall = Recall(task='binary', num_labels=output_labels).to(device)\n",
        "        precision = Precision(task='binary', num_labels=output_labels).to(device)\n",
        "        specificity = Specificity(task='binary', num_labels=output_labels).to(device)\n",
        "    else:\n",
        "        accuracy = Accuracy(task='multilabel', num_labels=output_labels).to(device)\n",
        "        f1score = F1Score(task='multilabel', num_labels=output_labels).to(device)\n",
        "        intersection = JaccardIndex(task='multilabel', num_labels=output_labels).to(device)\n",
        "        recall = Recall(task='multilabel', num_labels=output_labels).to(device)\n",
        "        precision = Precision(task='multilabel', num_labels=output_labels).to(device)\n",
        "        specificity = Specificity(task='multilabel', num_labels=output_labels).to(device)\n",
        "    # using model in evaluation\n",
        "    model.eval()\n",
        "    # if title is passed, use it before 'Check acc' and 'Got an accuracy...'\n",
        "    title = kwargs.get('title')\n",
        "    if title==None: title = ''\n",
        "    else: title = title+': '\n",
        "    # using tqdm.tqdm to show a progress bar\n",
        "    loop = tqdm(loader, desc=title+'Check acc')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for dictionary in loop:\n",
        "            image, label = dictionary\n",
        "            x, y = dictionary[image], dictionary[label]\n",
        "            x, y = x.to(device=device), y.to(device=device)\n",
        "            y = y.float()\n",
        "            pred = model(x)['out']\n",
        "            # Here 'output_labels == 2' means binary. We use this way because\n",
        "            # the model we are using returns a multilabel data, when we use\n",
        "            # this model with 2 labels, it becomes binary.\n",
        "            if output_labels == 2:\n",
        "                pred = pred[:,0]\n",
        "                y = y[:,0]\n",
        "                y = tf.center_crop(y, pred.shape[1:])\n",
        "            else:\n",
        "                y = tf.center_crop(y, pred.shape[2:])\n",
        "            acc = accuracy(pred, y)\n",
        "            f1 = f1score(pred, y)\n",
        "            iou = intersection(pred, y)\n",
        "            rec = recall(pred, y)\n",
        "            prec = precision(pred, y)\n",
        "            spec = specificity(pred, y)\n",
        "            # calculating loss\n",
        "            pred = (pred > 0.5).float()\n",
        "            loss = loss_fn(pred, y)\n",
        "            # deliting variables\n",
        "            loss_item = loss.item()\n",
        "            # freeing up space\n",
        "            del loss, pred, x, y, image, label, dictionary\n",
        "    # computing final metric\n",
        "    acc = accuracy.compute()\n",
        "    f1 = f1score.compute()\n",
        "    iou = intersection.compute()\n",
        "    rec = recall.compute()\n",
        "    prec = precision.compute()\n",
        "    spec = specificity.compute()\n",
        "    # freeing up space\n",
        "    del loader, loop\n",
        "\n",
        "    print('\\n'+title+f'Got an accuracy of {round(acc.item(), 4)}')\n",
        "    print('\\n'+title+f'F1 score: {round(f1.item(), 4)}'+'\\n')\n",
        "    # resiting metrics\n",
        "    accuracy.reset()\n",
        "    f1score.reset()\n",
        "    intersection.reset()\n",
        "    recall.reset()\n",
        "    precision.reset()\n",
        "    specificity.reset()\n",
        "    # returning model to training\n",
        "    model.train()\n",
        "\n",
        "    return acc.item(), f1.item(), iou.item(), rec.item(), prec.item(), spec.item(), loss_item\n",
        "\n",
        "\n",
        "#%% Saving images (only if the output are images)\n",
        "\n",
        "def save_predictions_as_imgs(loader, model, folder='saved_images',\n",
        "                             device='cuda' if torch.cuda.is_available() else 'cpu',\n",
        "                             **kwargs):\n",
        "    # If image is grayscale, if yes, we have to turn into rgb to save\n",
        "    gray = kwargs.get('gray')\n",
        "    # With model in evaluation\n",
        "    model.eval()\n",
        "    for idx, (dictionary) in enumerate(loader):\n",
        "        image, label = dictionary\n",
        "        x, y = dictionary[image], dictionary[label]\n",
        "        x = x.to(device=device)\n",
        "        y = y.to(dtype=torch.float32)\n",
        "        y = y.to(device=device)\n",
        "        with torch.no_grad():\n",
        "            pred = model(x)['out']\n",
        "            y = tf.center_crop(y, pred.shape[2:])\n",
        "            pred = (pred > 0.5).float()\n",
        "        # If image is grayscale, transforming to 'rgb' (utils.save_image needs)\n",
        "        if gray:\n",
        "            pred = torch.cat([pred,pred,pred],1)\n",
        "            y = y.unsqueeze(1)\n",
        "            y = torch.cat([y,y,y],1)\n",
        "            y = y.float()\n",
        "            y = tf.center_crop(y, pred.shape[2:])\n",
        "        save_image(pred, f'{folder}/pred_{idx}.png')\n",
        "        save_image(y, f'{folder}/y_{idx}.png')\n",
        "\n",
        "    model.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gl0SP2if825H",
        "outputId": "086aee61-86f9-4bd6-823e-06fb2a57bb28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-colab in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: google-auth==2.27.0 in /usr/local/lib/python3.10/dist-packages (from google-colab) (2.27.0)\n",
            "Requirement already satisfied: ipykernel==5.5.6 in /usr/local/lib/python3.10/dist-packages (from google-colab) (5.5.6)\n",
            "Requirement already satisfied: ipython==7.34.0 in /usr/local/lib/python3.10/dist-packages (from google-colab) (7.34.0)\n",
            "Requirement already satisfied: notebook==6.5.5 in /usr/local/lib/python3.10/dist-packages (from google-colab) (6.5.5)\n",
            "Requirement already satisfied: pandas==2.0.3 in /usr/local/lib/python3.10/dist-packages (from google-colab) (2.0.3)\n",
            "Requirement already satisfied: portpicker==1.5.2 in /usr/local/lib/python3.10/dist-packages (from google-colab) (1.5.2)\n",
            "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from google-colab) (2.31.0)\n",
            "Requirement already satisfied: tornado==6.3.3 in /usr/local/lib/python3.10/dist-packages (from google-colab) (6.3.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.27.0->google-colab) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.27.0->google-colab) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.27.0->google-colab) (4.9)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel==5.5.6->google-colab) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel==5.5.6->google-colab) (5.7.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel==5.5.6->google-colab) (6.1.12)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (67.7.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (4.9.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (3.1.4)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (0.20.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->google-colab) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->google-colab) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->google-colab) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->google-colab) (1.25.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker==1.5.2->google-colab) (5.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->google-colab) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->google-colab) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->google-colab) (1.26.19)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->google-colab) (2024.6.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython==7.34.0->google-colab) (0.8.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook==6.5.5->google-colab) (4.2.2)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook==6.5.5->google-colab) (0.2.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (2.1.5)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (0.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (24.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (1.3.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook==6.5.5->google-colab) (2.19.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook==6.5.5->google-colab) (4.19.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython==7.34.0->google-colab) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython==7.34.0->google-colab) (0.2.13)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth==2.27.0->google-colab) (0.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas==2.0.3->google-colab) (1.16.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook==6.5.5->google-colab) (21.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.5->google-colab) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.5->google-colab) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.5->google-colab) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.5->google-colab) (0.18.1)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.5->google-colab) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook==6.5.5->google-colab) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook==6.5.5->google-colab) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook==6.5.5->google-colab) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook==6.5.5->google-colab) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.5->google-colab) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.5->google-colab) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.5->google-colab) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.5->google-colab) (1.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkElNblXtDaz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ce0ae23278d84246a4daaaabdd79b449",
            "5043572aea3045dfa974184c5db49ced",
            "5bcdf60e997f4fb6b4da46f604f52f7d",
            "ca141c47bd8845669eeca932b0545d3e",
            "e80fea8db4814a33803f07498c8de418",
            "4d2f6d2b3c1341b897c24495dcf3a464",
            "23279135b6b64fbe870e8439ccbfb1ba",
            "6087cc9c09cc4e01ba87068b417ea239",
            "56179e4fb7b942ea9ad9b1fab51a7df5",
            "355f8a1096a346e183780f42356dd100",
            "45d28caccff344a1899de88a79e130cd"
          ]
        },
        "outputId": "c62394f1-cf6a-47a4-ce0b-1d612a6f36dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "\n",
            "- Loading Checkpoint...\n",
            "\n",
            "\n",
            "- Saving Images...\n",
            "\n",
            "image_names: ['image00.png', 'image01.png', 'image02.png', 'image03.png', 'image04.png', 'image05.png', 'image06.png', 'image07.png', 'image08.png', 'image09.png', 'image10.png', 'image11.png', 'image12.png', 'image13.png', 'image14.png', 'image15.png', 'image16.png', 'image17.png', 'image18.png', 'image19.png', 'image20.png', 'image21.png', 'image22.png', 'image23.png', 'image24.png', 'image25.png', 'image26.png', 'image27.png', 'image28.png', 'image29.png', 'image30.png', 'image31.png', 'image32.png', 'image33.png', 'image34.png', 'image35.png', 'image36.png', 'image37.png', 'image38.png', 'image39.png', 'image40.png', 'image41.png', 'image42.png', 'image43.png', 'image44.png', 'image45.png', 'image46.png', 'image47.png', 'image48.png', 'image49.png', 'image50.png', 'image51.png', 'image52.png', 'image53.png', 'image54.png', 'image55.png', 'image56.png', 'image57.png', 'image58.png', 'image59.png', 'image60.png', 'image61.png', 'image62.png', 'image63.png', 'image64.png', 'image65.png', 'image66.png', 'image67.png', 'image68.png', 'image69.png', 'image70.png', 'image71.png', 'image72.png', 'image73.png', 'image74.png', 'image75.png', 'image76.png', 'image77.png', 'image78.png', 'image79.png', 'image80.png', 'image81.png', 'image82.png']\n",
            "label: ['mask00.png', 'mask01.png', 'mask02.png', 'mask03.png', 'mask04.png', 'mask05.png', 'mask06.png', 'mask07.png', 'mask08.png', 'mask09.png', 'mask10.png', 'mask11.png', 'mask12.png', 'mask13.png', 'mask14.png', 'mask15.png', 'mask16.png', 'mask17.png', 'mask18.png', 'mask19.png', 'mask20.png', 'mask21.png', 'mask22.png', 'mask23.png', 'mask24.png', 'mask25.png', 'mask26.png', 'mask27.png', 'mask28.png', 'mask29.png', 'mask30.png', 'mask31.png', 'mask32.png', 'mask33.png', 'mask34.png', 'mask35.png', 'mask36.png', 'mask37.png', 'mask38.png', 'mask39.png', 'mask40.png', 'mask41.png', 'mask42.png', 'mask43.png', 'mask44.png', 'mask45.png', 'mask46.png', 'mask47.png', 'mask48.png', 'mask49.png', 'mask50.png', 'mask51.png', 'mask52.png', 'mask53.png', 'mask54.png', 'mask55.png', 'mask56.png', 'mask57.png', 'mask58.png', 'mask59.png', 'mask60.png', 'mask61.png', 'mask62.png', 'mask63.png', 'mask64.png', 'mask65.png', 'mask66.png', 'mask67.png', 'mask68.png', 'mask69.png', 'mask70.png', 'mask71.png', 'mask72.png', 'mask73.png', 'mask74.png', 'mask75.png', 'mask76.png', 'mask77.png', 'mask78.png', 'mask79.png', 'mask80.png', 'mask81.png', 'mask82.png']\n",
            "image_names: ['image00.png', 'image01.png', 'image02.png', 'image03.png', 'image04.png', 'image05.png', 'image06.png', 'image07.png', 'image08.png', 'image09.png', 'image10.png', 'image11.png', 'image12.png', 'image13.png', 'image14.png', 'image15.png', 'image16.png', 'image17.png', 'image18.png', 'image19.png', 'image20.png', 'image21.png', 'image22.png', 'image23.png', 'image24.png', 'image25.png', 'image26.png', 'image27.png', 'image28.png', 'image29.png', 'image30.png', 'image31.png', 'image32.png', 'image33.png', 'image34.png', 'image35.png', 'image36.png', 'image37.png', 'image38.png', 'image39.png', 'image40.png', 'image41.png', 'image42.png', 'image43.png', 'image44.png', 'image45.png', 'image46.png', 'image47.png', 'image48.png', 'image49.png', 'image50.png', 'image51.png', 'image52.png', 'image53.png', 'image54.png', 'image55.png', 'image56.png', 'image57.png', 'image58.png', 'image59.png', 'image60.png', 'image61.png', 'image62.png', 'image63.png']\n",
            "label: ['mask00.png', 'mask01.png', 'mask02.png', 'mask03.png', 'mask04.png', 'mask05.png', 'mask06.png', 'mask07.png', 'mask08.png', 'mask09.png', 'mask10.png', 'mask11.png', 'mask12.png', 'mask13.png', 'mask14.png', 'mask15.png', 'mask16.png', 'mask17.png', 'mask18.png', 'mask19.png', 'mask20.png', 'mask21.png', 'mask22.png', 'mask23.png', 'mask24.png', 'mask25.png', 'mask26.png', 'mask27.png', 'mask28.png', 'mask29.png', 'mask30.png', 'mask31.png', 'mask32.png', 'mask33.png', 'mask34.png', 'mask35.png', 'mask36.png', 'mask37.png', 'mask38.png', 'mask39.png', 'mask40.png', 'mask41.png', 'mask42.png', 'mask43.png', 'mask44.png', 'mask45.png', 'mask46.png', 'mask47.png', 'mask48.png', 'mask49.png', 'mask50.png', 'mask51.png', 'mask52.png', 'mask53.png', 'mask54.png', 'mask55.png', 'mask56.png', 'mask57.png', 'mask58.png', 'mask59.png', 'mask60.png', 'mask61.png', 'mask62.png', 'mask63.png']\n",
            "image_names: ['image00.png', 'image01.png', 'image02.png', 'image03.png', 'image04.png', 'image05.png', 'image06.png', 'image07.png', 'image08.png', 'image09.png', 'image10.png', 'image11.png', 'image12.png', 'image13.png', 'image14.png', 'image15.png', 'image16.png', 'image17.png', 'image18.png', 'image19.png', 'image20.png', 'image21.png', 'image22.png', 'image23.png', 'image24.png', 'image25.png', 'image26.png', 'image27.png', 'image28.png', 'image29.png', 'image30.png', 'image31.png', 'image32.png', 'image33.png', 'image34.png', 'image35.png', 'image36.png', 'image37.png', 'image38.png', 'image39.png', 'image40.png', 'image41.png', 'image42.png', 'image43.png', 'image44.png', 'image45.png', 'image46.png', 'image47.png', 'image48.png', 'image49.png', 'image50.png', 'image51.png', 'image52.png', 'image53.png', 'image54.png', 'image55.png', 'image56.png', 'image57.png', 'image58.png', 'image59.png', 'image60.png', 'image61.png', 'image62.png', 'image63.png', 'image64.png', 'image65.png', 'image66.png', 'image67.png', 'image68.png', 'image69.png', 'image70.png', 'image71.png', 'image72.png', 'image73.png', 'image74.png', 'image75.png', 'image76.png', 'image77.png', 'image78.png', 'image79.png', 'image80.png', 'image81.png', 'image82.png', 'image83.png', 'image84.png', 'image85.png', 'image86.png', 'image87.png', 'image88.png', 'image89.png']\n",
            "label: ['mask00.png', 'mask01.png', 'mask02.png', 'mask03.png', 'mask04.png', 'mask05.png', 'mask06.png', 'mask07.png', 'mask08.png', 'mask09.png', 'mask10.png', 'mask11.png', 'mask12.png', 'mask13.png', 'mask14.png', 'mask15.png', 'mask16.png', 'mask17.png', 'mask18.png', 'mask19.png', 'mask20.png', 'mask21.png', 'mask22.png', 'mask23.png', 'mask24.png', 'mask25.png', 'mask26.png', 'mask27.png', 'mask28.png', 'mask29.png', 'mask30.png', 'mask31.png', 'mask32.png', 'mask33.png', 'mask34.png', 'mask35.png', 'mask36.png', 'mask37.png', 'mask38.png', 'mask39.png', 'mask40.png', 'mask41.png', 'mask42.png', 'mask43.png', 'mask44.png', 'mask45.png', 'mask46.png', 'mask47.png', 'mask48.png', 'mask49.png', 'mask50.png', 'mask51.png', 'mask52.png', 'mask53.png', 'mask54.png', 'mask55.png', 'mask56.png', 'mask57.png', 'mask58.png', 'mask59.png', 'mask60.png', 'mask61.png', 'mask62.png', 'mask63.png', 'mask64.png', 'mask65.png', 'mask66.png', 'mask67.png', 'mask68.png', 'mask69.png', 'mask70.png', 'mask71.png', 'mask72.png', 'mask73.png', 'mask74.png', 'mask75.png', 'mask76.png', 'mask77.png', 'mask78.png', 'mask79.png', 'mask80.png', 'mask81.png', 'mask82.png', 'mask83.png', 'mask84.png', 'mask85.png', 'mask86.png', 'mask87.png', 'mask88.png', 'mask89.png']\n",
            "image_names: ['image00.png', 'image01.png', 'image02.png', 'image03.png', 'image04.png', 'image05.png', 'image06.png', 'image07.png', 'image08.png', 'image09.png', 'image10.png', 'image11.png', 'image12.png', 'image13.png', 'image14.png', 'image15.png', 'image16.png', 'image17.png', 'image18.png', 'image19.png', 'image20.png', 'image21.png', 'image22.png', 'image23.png', 'image24.png', 'image25.png', 'image26.png', 'image27.png', 'image28.png', 'image29.png', 'image30.png', 'image31.png', 'image32.png', 'image33.png', 'image34.png', 'image35.png']\n",
            "label: ['mask00.png', 'mask01.png', 'mask02.png', 'mask03.png', 'mask04.png', 'mask05.png', 'mask06.png', 'mask07.png', 'mask08.png', 'mask09.png', 'mask10.png', 'mask11.png', 'mask12.png', 'mask13.png', 'mask14.png', 'mask15.png', 'mask16.png', 'mask17.png', 'mask18.png', 'mask19.png', 'mask20.png', 'mask21.png', 'mask22.png', 'mask23.png', 'mask24.png', 'mask25.png', 'mask26.png', 'mask27.png', 'mask28.png', 'mask29.png', 'mask30.png', 'mask31.png', 'mask32.png', 'mask33.png', 'mask34.png', 'mask35.png']\n",
            "image_names: ['image00.png', 'image01.png', 'image02.png', 'image03.png', 'image04.png', 'image05.png', 'image06.png', 'image07.png', 'image08.png', 'image09.png', 'image10.png', 'image11.png', 'image12.png', 'image13.png']\n",
            "label: ['mask00.png', 'mask01.png', 'mask02.png', 'mask03.png', 'mask04.png', 'mask05.png', 'mask06.png', 'mask07.png', 'mask08.png', 'mask09.png', 'mask10.png', 'mask11.png', 'mask12.png', 'mask13.png']\n",
            "image_names: ['image00.png', 'image01.png', 'image02.png', 'image03.png', 'image04.png', 'image05.png', 'image06.png', 'image07.png', 'image08.png', 'image09.png', 'image10.png', 'image11.png', 'image12.png', 'image13.png', 'image14.png', 'image15.png', 'image16.png', 'image17.png', 'image18.png', 'image19.png', 'image20.png', 'image21.png', 'image22.png', 'image23.png', 'image24.png', 'image25.png', 'image26.png', 'image27.png', 'image28.png', 'image29.png', 'image30.png', 'image31.png', 'image32.png', 'image33.png', 'image34.png', 'image35.png', 'image36.png', 'image37.png', 'image38.png', 'image39.png']\n",
            "label: ['mask00.png', 'mask01.png', 'mask02.png', 'mask03.png', 'mask04.png', 'mask05.png', 'mask06.png', 'mask07.png', 'mask08.png', 'mask09.png', 'mask10.png', 'mask11.png', 'mask12.png', 'mask13.png', 'mask14.png', 'mask15.png', 'mask16.png', 'mask17.png', 'mask18.png', 'mask19.png', 'mask20.png', 'mask21.png', 'mask22.png', 'mask23.png', 'mask24.png', 'mask25.png', 'mask26.png', 'mask27.png', 'mask28.png', 'mask29.png', 'mask30.png', 'mask31.png', 'mask32.png', 'mask33.png', 'mask34.png', 'mask35.png', 'mask36.png', 'mask37.png', 'mask38.png', 'mask39.png']\n",
            "image_names: ['image00.png', 'image01.png', 'image02.png', 'image03.png', 'image04.png']\n",
            "label: ['mask00.png', 'mask01.png', 'mask02.png', 'mask03.png', 'mask04.png']\n",
            "image_names: ['image00.png', 'image01.png', 'image02.png', 'image03.png', 'image04.png', 'image05.png', 'image06.png', 'image07.png', 'image08.png', 'image09.png', 'image10.png', 'image11.png', 'image12.png', 'image13.png', 'image14.png', 'image15.png', 'image16.png', 'image17.png', 'image18.png', 'image19.png', 'image20.png', 'image21.png', 'image22.png', 'image23.png', 'image24.png', 'image25.png', 'image26.png', 'image27.png', 'image28.png', 'image29.png', 'image30.png', 'image31.png', 'image32.png', 'image33.png', 'image34.png', 'image35.png', 'image36.png', 'image37.png', 'image38.png', 'image39.png', 'image40.png', 'image41.png', 'image42.png', 'image43.png', 'image44.png', 'image45.png', 'image46.png', 'image47.png', 'image48.png', 'image49.png', 'image50.png', 'image51.png', 'image52.png', 'image53.png', 'image54.png', 'image55.png', 'image56.png', 'image57.png', 'image58.png', 'image59.png', 'image60.png', 'image61.png', 'image62.png', 'image63.png', 'image64.png', 'image65.png', 'image66.png', 'image67.png', 'image68.png', 'image69.png', 'image70.png', 'image71.png', 'image72.png', 'image73.png', 'image74.png', 'image75.png', 'image76.png', 'image77.png', 'image78.png', 'image79.png', 'image80.png', 'image81.png', 'image82.png']\n",
            "label: ['mask00.png', 'mask01.png', 'mask02.png', 'mask03.png', 'mask04.png', 'mask05.png', 'mask06.png', 'mask07.png', 'mask08.png', 'mask09.png', 'mask10.png', 'mask11.png', 'mask12.png', 'mask13.png', 'mask14.png', 'mask15.png', 'mask16.png', 'mask17.png', 'mask18.png', 'mask19.png', 'mask20.png', 'mask21.png', 'mask22.png', 'mask23.png', 'mask24.png', 'mask25.png', 'mask26.png', 'mask27.png', 'mask28.png', 'mask29.png', 'mask30.png', 'mask31.png', 'mask32.png', 'mask33.png', 'mask34.png', 'mask35.png', 'mask36.png', 'mask37.png', 'mask38.png', 'mask39.png', 'mask40.png', 'mask41.png', 'mask42.png', 'mask43.png', 'mask44.png', 'mask45.png', 'mask46.png', 'mask47.png', 'mask48.png', 'mask49.png', 'mask50.png', 'mask51.png', 'mask52.png', 'mask53.png', 'mask54.png', 'mask55.png', 'mask56.png', 'mask57.png', 'mask58.png', 'mask59.png', 'mask60.png', 'mask61.png', 'mask62.png', 'mask63.png', 'mask64.png', 'mask65.png', 'mask66.png', 'mask67.png', 'mask68.png', 'mask69.png', 'mask70.png', 'mask71.png', 'mask72.png', 'mask73.png', 'mask74.png', 'mask75.png', 'mask76.png', 'mask77.png', 'mask78.png', 'mask79.png', 'mask80.png', 'mask81.png', 'mask82.png']\n",
            "image_names: ['image00.png', 'image01.png', 'image02.png', 'image03.png', 'image04.png', 'image05.png', 'image06.png', 'image07.png', 'image08.png', 'image09.png', 'image10.png', 'image11.png', 'image12.png', 'image13.png', 'image14.png', 'image15.png', 'image16.png', 'image17.png', 'image18.png', 'image19.png', 'image20.png', 'image21.png', 'image22.png', 'image23.png', 'image24.png', 'image25.png', 'image26.png', 'image27.png', 'image28.png', 'image29.png', 'image30.png', 'image31.png', 'image32.png', 'image33.png', 'image34.png', 'image35.png', 'image36.png', 'image37.png', 'image38.png', 'image39.png', 'image40.png', 'image41.png', 'image42.png', 'image43.png', 'image44.png', 'image45.png', 'image46.png', 'image47.png', 'image48.png', 'image49.png', 'image50.png', 'image51.png', 'image52.png', 'image53.png', 'image54.png', 'image55.png', 'image56.png', 'image57.png', 'image58.png', 'image59.png', 'image60.png', 'image61.png', 'image62.png', 'image63.png', 'image64.png', 'image65.png', 'image66.png', 'image67.png', 'image68.png', 'image69.png', 'image70.png', 'image71.png', 'image72.png', 'image73.png', 'image74.png', 'image75.png', 'image76.png', 'image77.png', 'image78.png', 'image79.png']\n",
            "label: ['mask00.png', 'mask01.png', 'mask02.png', 'mask03.png', 'mask04.png', 'mask05.png', 'mask06.png', 'mask07.png', 'mask08.png', 'mask09.png', 'mask10.png', 'mask11.png', 'mask12.png', 'mask13.png', 'mask14.png', 'mask15.png', 'mask16.png', 'mask17.png', 'mask18.png', 'mask19.png', 'mask20.png', 'mask21.png', 'mask22.png', 'mask23.png', 'mask24.png', 'mask25.png', 'mask26.png', 'mask27.png', 'mask28.png', 'mask29.png', 'mask30.png', 'mask31.png', 'mask32.png', 'mask33.png', 'mask34.png', 'mask35.png', 'mask36.png', 'mask37.png', 'mask38.png', 'mask39.png', 'mask40.png', 'mask41.png', 'mask42.png', 'mask43.png', 'mask44.png', 'mask45.png', 'mask46.png', 'mask47.png', 'mask48.png', 'mask49.png', 'mask50.png', 'mask51.png', 'mask52.png', 'mask53.png', 'mask54.png', 'mask55.png', 'mask56.png', 'mask57.png', 'mask58.png', 'mask59.png', 'mask60.png', 'mask61.png', 'mask62.png', 'mask63.png', 'mask64.png', 'mask65.png', 'mask66.png', 'mask67.png', 'mask68.png', 'mask69.png', 'mask70.png', 'mask71.png', 'mask72.png', 'mask73.png', 'mask74.png', 'mask75.png', 'mask76.png', 'mask77.png', 'mask78.png', 'mask79.png']\n",
            "image_names: ['image00.png', 'image01.png', 'image02.png', 'image03.png', 'image04.png', 'image05.png', 'image06.png', 'image07.png', 'image08.png', 'image09.png', 'image10.png', 'image11.png', 'image12.png', 'image13.png', 'image14.png', 'image15.png', 'image16.png', 'image17.png', 'image18.png', 'image19.png', 'image20.png', 'image21.png', 'image22.png', 'image23.png', 'image24.png', 'image25.png', 'image26.png', 'image27.png', 'image28.png', 'image29.png', 'image30.png', 'image31.png', 'image32.png', 'image33.png', 'image34.png', 'image35.png', 'image36.png', 'image37.png', 'image38.png', 'image39.png', 'image40.png', 'image41.png', 'image42.png', 'image43.png']\n",
            "label: ['mask00.png', 'mask01.png', 'mask02.png', 'mask03.png', 'mask04.png', 'mask05.png', 'mask06.png', 'mask07.png', 'mask08.png', 'mask09.png', 'mask10.png', 'mask11.png', 'mask12.png', 'mask13.png', 'mask14.png', 'mask15.png', 'mask16.png', 'mask17.png', 'mask18.png', 'mask19.png', 'mask20.png', 'mask21.png', 'mask22.png', 'mask23.png', 'mask24.png', 'mask25.png', 'mask26.png', 'mask27.png', 'mask28.png', 'mask29.png', 'mask30.png', 'mask31.png', 'mask32.png', 'mask33.png', 'mask34.png', 'mask35.png', 'mask36.png', 'mask37.png', 'mask38.png', 'mask39.png', 'mask40.png', 'mask41.png', 'mask42.png', 'mask43.png']\n",
            "image_names: ['image00.png', 'image01.png', 'image02.png', 'image03.png', 'image04.png', 'image05.png', 'image06.png', 'image07.png', 'image08.png', 'image09.png', 'image10.png', 'image11.png', 'image12.png', 'image13.png', 'image14.png', 'image15.png', 'image16.png', 'image17.png', 'image18.png', 'image19.png', 'image20.png', 'image21.png', 'image22.png', 'image23.png', 'image24.png', 'image25.png', 'image26.png', 'image27.png', 'image28.png', 'image29.png', 'image30.png', 'image31.png', 'image32.png', 'image33.png', 'image34.png', 'image35.png', 'image36.png', 'image37.png', 'image38.png', 'image39.png', 'image40.png', 'image41.png']\n",
            "label: ['mask00.png', 'mask01.png', 'mask02.png', 'mask03.png', 'mask04.png', 'mask05.png', 'mask06.png', 'mask07.png', 'mask08.png', 'mask09.png', 'mask10.png', 'mask11.png', 'mask12.png', 'mask13.png', 'mask14.png', 'mask15.png', 'mask16.png', 'mask17.png', 'mask18.png', 'mask19.png', 'mask20.png', 'mask21.png', 'mask22.png', 'mask23.png', 'mask24.png', 'mask25.png', 'mask26.png', 'mask27.png', 'mask28.png', 'mask29.png', 'mask30.png', 'mask31.png', 'mask32.png', 'mask33.png', 'mask34.png', 'mask35.png', 'mask36.png', 'mask37.png', 'mask38.png', 'mask39.png', 'mask40.png', 'mask41.png']\n",
            "image_names: ['image00.png', 'image01.png', 'image02.png', 'image03.png', 'image04.png', 'image05.png', 'image06.png', 'image07.png', 'image08.png', 'image09.png', 'image10.png', 'image11.png', 'image12.png', 'image13.png', 'image14.png', 'image15.png', 'image16.png', 'image17.png', 'image18.png', 'image19.png', 'image20.png', 'image21.png', 'image22.png', 'image23.png', 'image24.png', 'image25.png', 'image26.png', 'image27.png', 'image28.png', 'image29.png', 'image30.png', 'image31.png', 'image32.png', 'image33.png', 'image34.png', 'image35.png', 'image36.png', 'image37.png', 'image38.png', 'image39.png', 'image40.png', 'image41.png', 'image42.png', 'image43.png', 'image44.png', 'image45.png', 'image46.png']\n",
            "label: ['mask00.png', 'mask01.png', 'mask02.png', 'mask03.png', 'mask04.png', 'mask05.png', 'mask06.png', 'mask07.png', 'mask08.png', 'mask09.png', 'mask10.png', 'mask11.png', 'mask12.png', 'mask13.png', 'mask14.png', 'mask15.png', 'mask16.png', 'mask17.png', 'mask18.png', 'mask19.png', 'mask20.png', 'mask21.png', 'mask22.png', 'mask23.png', 'mask24.png', 'mask25.png', 'mask26.png', 'mask27.png', 'mask28.png', 'mask29.png', 'mask30.png', 'mask31.png', 'mask32.png', 'mask33.png', 'mask34.png', 'mask35.png', 'mask36.png', 'mask37.png', 'mask38.png', 'mask39.png', 'mask40.png', 'mask41.png', 'mask42.png', 'mask43.png', 'mask44.png', 'mask45.png', 'mask46.png']\n",
            "image_names: ['image00.png', 'image01.png', 'image02.png', 'image03.png', 'image04.png', 'image05.png', 'image06.png', 'image07.png', 'image08.png']\n",
            "label: ['mask00.png', 'mask01.png', 'mask02.png', 'mask03.png', 'mask04.png', 'mask05.png', 'mask06.png', 'mask07.png', 'mask08.png']\n",
            "image_names: ['image00.png', 'image01.png', 'image02.png', 'image03.png', 'image04.png', 'image05.png', 'image06.png', 'image07.png']\n",
            "label: ['mask00.png', 'mask01.png', 'mask02.png', 'mask03.png', 'mask04.png', 'mask05.png', 'mask06.png', 'mask07.png']\n",
            "image_names: ['image00.png', 'image01.png', 'image02.png', 'image03.png', 'image04.png', 'image05.png', 'image06.png', 'image07.png', 'image08.png', 'image09.png', 'image10.png', 'image11.png', 'image12.png', 'image13.png', 'image14.png', 'image15.png', 'image16.png', 'image17.png', 'image18.png', 'image19.png', 'image20.png', 'image21.png', 'image22.png', 'image23.png', 'image24.png', 'image25.png', 'image26.png', 'image27.png', 'image28.png', 'image29.png', 'image30.png', 'image31.png', 'image32.png', 'image33.png', 'image34.png', 'image35.png', 'image36.png', 'image37.png', 'image38.png', 'image39.png', 'image40.png', 'image41.png', 'image42.png', 'image43.png', 'image44.png', 'image45.png', 'image46.png', 'image47.png', 'image48.png', 'image49.png', 'image50.png', 'image51.png', 'image52.png', 'image53.png', 'image54.png', 'image55.png']\n",
            "label: ['mask00.png', 'mask01.png', 'mask02.png', 'mask03.png', 'mask04.png', 'mask05.png', 'mask06.png', 'mask07.png', 'mask08.png', 'mask09.png', 'mask10.png', 'mask11.png', 'mask12.png', 'mask13.png', 'mask14.png', 'mask15.png', 'mask16.png', 'mask17.png', 'mask18.png', 'mask19.png', 'mask20.png', 'mask21.png', 'mask22.png', 'mask23.png', 'mask24.png', 'mask25.png', 'mask26.png', 'mask27.png', 'mask28.png', 'mask29.png', 'mask30.png', 'mask31.png', 'mask32.png', 'mask33.png', 'mask34.png', 'mask35.png', 'mask36.png', 'mask37.png', 'mask38.png', 'mask39.png', 'mask40.png', 'mask41.png', 'mask42.png', 'mask43.png', 'mask44.png', 'mask45.png', 'mask46.png', 'mask47.png', 'mask48.png', 'mask49.png', 'mask50.png', 'mask51.png', 'mask52.png', 'mask53.png', 'mask54.png', 'mask55.png']\n",
            "image_names: ['image00.png', 'image01.png', 'image02.png', 'image03.png', 'image04.png', 'image05.png', 'image06.png', 'image07.png', 'image08.png', 'image09.png']\n",
            "label: ['mask00.png', 'mask01.png', 'mask02.png', 'mask03.png', 'mask04.png', 'mask05.png', 'mask06.png', 'mask07.png', 'mask08.png', 'mask09.png']\n",
            "image_names: ['image00.png', 'image01.png', 'image02.png', 'image03.png', 'image04.png', 'image05.png', 'image06.png', 'image07.png', 'image08.png', 'image09.png', 'image10.png', 'image11.png', 'image12.png', 'image13.png', 'image14.png', 'image15.png', 'image16.png', 'image17.png', 'image18.png', 'image19.png', 'image20.png', 'image21.png', 'image22.png', 'image23.png', 'image24.png', 'image25.png', 'image26.png', 'image27.png', 'image28.png', 'image29.png', 'image30.png', 'image31.png', 'image32.png', 'image33.png', 'image34.png', 'image35.png', 'image36.png', 'image37.png', 'image38.png', 'image39.png', 'image40.png', 'image41.png', 'image42.png', 'image43.png', 'image44.png', 'image45.png', 'image46.png', 'image47.png', 'image48.png', 'image49.png', 'image50.png', 'image51.png', 'image52.png', 'image53.png', 'image54.png', 'image55.png', 'image56.png', 'image57.png', 'image58.png', 'image59.png', 'image60.png', 'image61.png', 'image62.png', 'image63.png', 'image64.png', 'image65.png', 'image66.png', 'image67.png', 'image68.png', 'image69.png', 'image70.png', 'image71.png', 'image72.png', 'image73.png', 'image74.png', 'image75.png', 'image76.png', 'image77.png', 'image78.png', 'image79.png', 'image80.png', 'image81.png', 'image82.png']\n",
            "label: ['mask00.png', 'mask01.png', 'mask02.png', 'mask03.png', 'mask04.png', 'mask05.png', 'mask06.png', 'mask07.png', 'mask08.png', 'mask09.png', 'mask10.png', 'mask11.png', 'mask12.png', 'mask13.png', 'mask14.png', 'mask15.png', 'mask16.png', 'mask17.png', 'mask18.png', 'mask19.png', 'mask20.png', 'mask21.png', 'mask22.png', 'mask23.png', 'mask24.png', 'mask25.png', 'mask26.png', 'mask27.png', 'mask28.png', 'mask29.png', 'mask30.png', 'mask31.png', 'mask32.png', 'mask33.png', 'mask34.png', 'mask35.png', 'mask36.png', 'mask37.png', 'mask38.png', 'mask39.png', 'mask40.png', 'mask41.png', 'mask42.png', 'mask43.png', 'mask44.png', 'mask45.png', 'mask46.png', 'mask47.png', 'mask48.png', 'mask49.png', 'mask50.png', 'mask51.png', 'mask52.png', 'mask53.png', 'mask54.png', 'mask55.png', 'mask56.png', 'mask57.png', 'mask58.png', 'mask59.png', 'mask60.png', 'mask61.png', 'mask62.png', 'mask63.png', 'mask64.png', 'mask65.png', 'mask66.png', 'mask67.png', 'mask68.png', 'mask69.png', 'mask70.png', 'mask71.png', 'mask72.png', 'mask73.png', 'mask74.png', 'mask75.png', 'mask76.png', 'mask77.png', 'mask78.png', 'mask79.png', 'mask80.png', 'mask81.png', 'mask82.png']\n",
            "image_names: ['image00.png', 'image01.png', 'image02.png', 'image03.png', 'image04.png', 'image05.png', 'image06.png', 'image07.png', 'image08.png', 'image09.png', 'image10.png', 'image11.png', 'image12.png', 'image13.png', 'image14.png', 'image15.png', 'image16.png', 'image17.png', 'image18.png', 'image19.png', 'image20.png', 'image21.png', 'image22.png', 'image23.png', 'image24.png', 'image25.png', 'image26.png', 'image27.png', 'image28.png', 'image29.png', 'image30.png', 'image31.png', 'image32.png', 'image33.png', 'image34.png', 'image35.png', 'image36.png', 'image37.png', 'image38.png', 'image39.png', 'image40.png', 'image41.png', 'image42.png', 'image43.png', 'image44.png', 'image45.png', 'image46.png', 'image47.png', 'image48.png', 'image49.png', 'image50.png', 'image51.png', 'image52.png', 'image53.png', 'image54.png', 'image55.png', 'image56.png', 'image57.png', 'image58.png', 'image59.png', 'image60.png', 'image61.png']\n",
            "label: ['mask00.png', 'mask01.png', 'mask02.png', 'mask03.png', 'mask04.png', 'mask05.png', 'mask06.png', 'mask07.png', 'mask08.png', 'mask09.png', 'mask10.png', 'mask11.png', 'mask12.png', 'mask13.png', 'mask14.png', 'mask15.png', 'mask16.png', 'mask17.png', 'mask18.png', 'mask19.png', 'mask20.png', 'mask21.png', 'mask22.png', 'mask23.png', 'mask24.png', 'mask25.png', 'mask26.png', 'mask27.png', 'mask28.png', 'mask29.png', 'mask30.png', 'mask31.png', 'mask32.png', 'mask33.png', 'mask34.png', 'mask35.png', 'mask36.png', 'mask37.png', 'mask38.png', 'mask39.png', 'mask40.png', 'mask41.png', 'mask42.png', 'mask43.png', 'mask44.png', 'mask45.png', 'mask46.png', 'mask47.png', 'mask48.png', 'mask49.png', 'mask50.png', 'mask51.png', 'mask52.png', 'mask53.png', 'mask54.png', 'mask55.png', 'mask56.png', 'mask57.png', 'mask58.png', 'mask59.png', 'mask60.png', 'mask61.png']\n",
            "image_names: ['image00.png', 'image01.png', 'image02.png', 'image03.png', 'image04.png', 'image05.png', 'image06.png', 'image07.png', 'image08.png', 'image09.png', 'image10.png', 'image11.png', 'image12.png', 'image13.png', 'image14.png', 'image15.png', 'image16.png', 'image17.png', 'image18.png', 'image19.png', 'image20.png', 'image21.png', 'image22.png', 'image23.png', 'image24.png', 'image25.png', 'image26.png', 'image27.png', 'image28.png', 'image29.png', 'image30.png', 'image31.png', 'image32.png', 'image33.png', 'image34.png', 'image35.png', 'image36.png', 'image37.png', 'image38.png', 'image39.png', 'image40.png', 'image41.png', 'image42.png', 'image43.png', 'image44.png', 'image45.png', 'image46.png', 'image47.png', 'image48.png', 'image49.png', 'image50.png', 'image51.png', 'image52.png', 'image53.png', 'image54.png', 'image55.png', 'image56.png', 'image57.png', 'image58.png', 'image59.png', 'image60.png', 'image61.png', 'image62.png', 'image63.png', 'image64.png', 'image65.png', 'image66.png', 'image67.png', 'image68.png', 'image69.png', 'image70.png', 'image71.png', 'image72.png', 'image73.png', 'image74.png', 'image75.png', 'image76.png', 'image77.png', 'image78.png', 'image79.png', 'image80.png', 'image81.png', 'image82.png', 'image83.png', 'image84.png', 'image85.png']\n",
            "label: ['mask00.png', 'mask01.png', 'mask02.png', 'mask03.png', 'mask04.png', 'mask05.png', 'mask06.png', 'mask07.png', 'mask08.png', 'mask09.png', 'mask10.png', 'mask11.png', 'mask12.png', 'mask13.png', 'mask14.png', 'mask15.png', 'mask16.png', 'mask17.png', 'mask18.png', 'mask19.png', 'mask20.png', 'mask21.png', 'mask22.png', 'mask23.png', 'mask24.png', 'mask25.png', 'mask26.png', 'mask27.png', 'mask28.png', 'mask29.png', 'mask30.png', 'mask31.png', 'mask32.png', 'mask33.png', 'mask34.png', 'mask35.png', 'mask36.png', 'mask37.png', 'mask38.png', 'mask39.png', 'mask40.png', 'mask41.png', 'mask42.png', 'mask43.png', 'mask44.png', 'mask45.png', 'mask46.png', 'mask47.png', 'mask48.png', 'mask49.png', 'mask50.png', 'mask51.png', 'mask52.png', 'mask53.png', 'mask54.png', 'mask55.png', 'mask56.png', 'mask57.png', 'mask58.png', 'mask59.png', 'mask60.png', 'mask61.png', 'mask62.png', 'mask63.png', 'mask64.png', 'mask65.png', 'mask66.png', 'mask67.png', 'mask68.png', 'mask69.png', 'mask70.png', 'mask71.png', 'mask72.png', 'mask73.png', 'mask74.png', 'mask75.png', 'mask76.png', 'mask77.png', 'mask78.png', 'mask79.png', 'mask80.png', 'mask81.png', 'mask82.png', 'mask83.png', 'mask84.png', 'mask85.png']\n",
            "image_names: ['image00.png', 'image01.png', 'image02.png', 'image03.png', 'image04.png', 'image05.png', 'image06.png', 'image07.png', 'image08.png', 'image09.png', 'image10.png', 'image11.png', 'image12.png', 'image13.png', 'image14.png', 'image15.png', 'image16.png', 'image17.png', 'image18.png', 'image19.png', 'image20.png', 'image21.png', 'image22.png', 'image23.png', 'image24.png', 'image25.png', 'image26.png', 'image27.png', 'image28.png', 'image29.png', 'image30.png', 'image31.png', 'image32.png', 'image33.png', 'image34.png', 'image35.png', 'image36.png', 'image37.png', 'image38.png', 'image39.png', 'image40.png', 'image41.png', 'image42.png', 'image43.png', 'image44.png', 'image45.png', 'image46.png', 'image47.png', 'image48.png', 'image49.png', 'image50.png', 'image51.png', 'image52.png', 'image53.png', 'image54.png', 'image55.png', 'image56.png', 'image57.png', 'image58.png', 'image59.png', 'image60.png', 'image61.png', 'image62.png', 'image63.png', 'image64.png', 'image65.png', 'image66.png', 'image67.png', 'image68.png', 'image69.png', 'image70.png', 'image71.png', 'image72.png', 'image73.png', 'image74.png', 'image75.png', 'image76.png', 'image77.png', 'image78.png', 'image79.png', 'image80.png']\n",
            "label: ['mask00.png', 'mask01.png', 'mask02.png', 'mask03.png', 'mask04.png', 'mask05.png', 'mask06.png', 'mask07.png', 'mask08.png', 'mask09.png', 'mask10.png', 'mask11.png', 'mask12.png', 'mask13.png', 'mask14.png', 'mask15.png', 'mask16.png', 'mask17.png', 'mask18.png', 'mask19.png', 'mask20.png', 'mask21.png', 'mask22.png', 'mask23.png', 'mask24.png', 'mask25.png', 'mask26.png', 'mask27.png', 'mask28.png', 'mask29.png', 'mask30.png', 'mask31.png', 'mask32.png', 'mask33.png', 'mask34.png', 'mask35.png', 'mask36.png', 'mask37.png', 'mask38.png', 'mask39.png', 'mask40.png', 'mask41.png', 'mask42.png', 'mask43.png', 'mask44.png', 'mask45.png', 'mask46.png', 'mask47.png', 'mask48.png', 'mask49.png', 'mask50.png', 'mask51.png', 'mask52.png', 'mask53.png', 'mask54.png', 'mask55.png', 'mask56.png', 'mask57.png', 'mask58.png', 'mask59.png', 'mask60.png', 'mask61.png', 'mask62.png', 'mask63.png', 'mask64.png', 'mask65.png', 'mask66.png', 'mask67.png', 'mask68.png', 'mask69.png', 'mask70.png', 'mask71.png', 'mask72.png', 'mask73.png', 'mask74.png', 'mask75.png', 'mask76.png', 'mask77.png', 'mask78.png', 'mask79.png', 'mask80.png']\n",
            "image_names: ['image00.png', 'image01.png', 'image02.png', 'image03.png', 'image04.png', 'image05.png', 'image06.png', 'image07.png', 'image08.png', 'image09.png', 'image10.png', 'image11.png', 'image12.png', 'image13.png', 'image14.png', 'image15.png', 'image16.png', 'image17.png', 'image18.png', 'image19.png', 'image20.png', 'image21.png', 'image22.png', 'image23.png', 'image24.png', 'image25.png', 'image26.png', 'image27.png', 'image28.png', 'image29.png', 'image30.png', 'image31.png', 'image32.png', 'image33.png', 'image34.png', 'image35.png', 'image36.png', 'image37.png', 'image38.png', 'image39.png', 'image40.png', 'image41.png', 'image42.png', 'image43.png', 'image44.png', 'image45.png', 'image46.png', 'image47.png', 'image48.png', 'image49.png', 'image50.png', 'image51.png', 'image52.png', 'image53.png', 'image54.png', 'image55.png', 'image56.png', 'image57.png', 'image58.png', 'image59.png', 'image60.png', 'image61.png', 'image62.png', 'image63.png', 'image64.png', 'image65.png', 'image66.png', 'image67.png', 'image68.png', 'image69.png', 'image70.png', 'image71.png', 'image72.png', 'image73.png', 'image74.png', 'image75.png', 'image76.png', 'image77.png', 'image78.png', 'image79.png', 'image80.png', 'image81.png', 'image82.png', 'image83.png', 'image84.png', 'image85.png', 'image86.png', 'image87.png', 'image88.png', 'image89.png']\n",
            "label: ['mask00.png', 'mask01.png', 'mask02.png', 'mask03.png', 'mask04.png', 'mask05.png', 'mask06.png', 'mask07.png', 'mask08.png', 'mask09.png', 'mask10.png', 'mask11.png', 'mask12.png', 'mask13.png', 'mask14.png', 'mask15.png', 'mask16.png', 'mask17.png', 'mask18.png', 'mask19.png', 'mask20.png', 'mask21.png', 'mask22.png', 'mask23.png', 'mask24.png', 'mask25.png', 'mask26.png', 'mask27.png', 'mask28.png', 'mask29.png', 'mask30.png', 'mask31.png', 'mask32.png', 'mask33.png', 'mask34.png', 'mask35.png', 'mask36.png', 'mask37.png', 'mask38.png', 'mask39.png', 'mask40.png', 'mask41.png', 'mask42.png', 'mask43.png', 'mask44.png', 'mask45.png', 'mask46.png', 'mask47.png', 'mask48.png', 'mask49.png', 'mask50.png', 'mask51.png', 'mask52.png', 'mask53.png', 'mask54.png', 'mask55.png', 'mask56.png', 'mask57.png', 'mask58.png', 'mask59.png', 'mask60.png', 'mask61.png', 'mask62.png', 'mask63.png', 'mask64.png', 'mask65.png', 'mask66.png', 'mask67.png', 'mask68.png', 'mask69.png', 'mask70.png', 'mask71.png', 'mask72.png', 'mask73.png', 'mask74.png', 'mask75.png', 'mask76.png', 'mask77.png', 'mask78.png', 'mask79.png', 'mask80.png', 'mask81.png', 'mask82.png', 'mask83.png', 'mask84.png', 'mask85.png', 'mask86.png', 'mask87.png', 'mask88.png', 'mask89.png']\n",
            "['image00.png', 'image01.png', 'image02.png', 'image03.png', 'image04.png', 'image05.png', 'image06.png', 'image07.png', 'image08.png', 'image09.png', 'image10.png', 'image11.png', 'image12.png', 'image13.png', 'image14.png', 'image15.png', 'image16.png', 'image17.png', 'image18.png', 'image19.png', 'image20.png', 'image21.png', 'image22.png', 'image23.png', 'image24.png', 'image25.png', 'image26.png', 'image27.png', 'image28.png', 'image29.png', 'image30.png', 'image31.png', 'image32.png', 'image33.png', 'image34.png', 'image35.png', 'image36.png', 'image37.png', 'image38.png', 'image39.png', 'image40.png', 'image41.png', 'image42.png', 'image43.png', 'image44.png', 'image45.png', 'image46.png', 'image47.png', 'image48.png', 'image49.png', 'image50.png', 'image51.png', 'image52.png', 'image53.png', 'image54.png', 'image55.png', 'image56.png', 'image57.png', 'image58.png', 'image59.png', 'image60.png', 'image61.png', 'image62.png', 'image63.png', 'image64.png', 'image65.png', 'image66.png', 'image67.png', 'image68.png', 'image69.png', 'image70.png', 'image71.png', 'image72.png', 'image73.png', 'image74.png', 'image75.png', 'image76.png', 'image77.png', 'image78.png', 'image79.png', 'image80.png', 'image81.png', 'image82.png', 'image83.png', 'image84.png', 'image85.png', 'image86.png', 'image87.png', 'image88.png', 'image89.png']\n",
            "['mask00.png', 'mask01.png', 'mask02.png', 'mask03.png', 'mask04.png', 'mask05.png', 'mask06.png', 'mask07.png', 'mask08.png', 'mask09.png', 'mask10.png', 'mask11.png', 'mask12.png', 'mask13.png', 'mask14.png', 'mask15.png', 'mask16.png', 'mask17.png', 'mask18.png', 'mask19.png', 'mask20.png', 'mask21.png', 'mask22.png', 'mask23.png', 'mask24.png', 'mask25.png', 'mask26.png', 'mask27.png', 'mask28.png', 'mask29.png', 'mask30.png', 'mask31.png', 'mask32.png', 'mask33.png', 'mask34.png', 'mask35.png', 'mask36.png', 'mask37.png', 'mask38.png', 'mask39.png', 'mask40.png', 'mask41.png', 'mask42.png', 'mask43.png', 'mask44.png', 'mask45.png', 'mask46.png', 'mask47.png', 'mask48.png', 'mask49.png', 'mask50.png', 'mask51.png', 'mask52.png', 'mask53.png', 'mask54.png', 'mask55.png', 'mask56.png', 'mask57.png', 'mask58.png', 'mask59.png', 'mask60.png', 'mask61.png', 'mask62.png', 'mask63.png', 'mask64.png', 'mask65.png', 'mask66.png', 'mask67.png', 'mask68.png', 'mask69.png', 'mask70.png', 'mask71.png', 'mask72.png', 'mask73.png', 'mask74.png', 'mask75.png', 'mask76.png', 'mask77.png', 'mask78.png', 'mask79.png', 'mask80.png', 'mask81.png', 'mask82.png', 'mask83.png', 'mask84.png', 'mask85.png', 'mask86.png', 'mask87.png', 'mask88.png', 'mask89.png']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/90 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce0ae23278d84246a4daaaabdd79b449"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-e70c224ca631>:260: RuntimeWarning: invalid value encountered in divide\n",
            "  image = image/(np.max(image)/255)\n",
            "<ipython-input-9-e70c224ca631>:261: RuntimeWarning: invalid value encountered in cast\n",
            "  return np.array(image, np.uint8)\n",
            "<ipython-input-9-e70c224ca631>:260: RuntimeWarning: invalid value encountered in divide\n",
            "  image = image/(np.max(image)/255)\n",
            "<ipython-input-9-e70c224ca631>:261: RuntimeWarning: invalid value encountered in cast\n",
            "  return np.array(image, np.uint8)\n",
            "<ipython-input-9-e70c224ca631>:260: RuntimeWarning: invalid value encountered in divide\n",
            "  image = image/(np.max(image)/255)\n",
            "<ipython-input-9-e70c224ca631>:261: RuntimeWarning: invalid value encountered in cast\n",
            "  return np.array(image, np.uint8)\n",
            "<ipython-input-9-e70c224ca631>:260: RuntimeWarning: invalid value encountered in divide\n",
            "  image = image/(np.max(image)/255)\n",
            "<ipython-input-9-e70c224ca631>:261: RuntimeWarning: invalid value encountered in cast\n",
            "  return np.array(image, np.uint8)\n",
            "<ipython-input-9-e70c224ca631>:260: RuntimeWarning: invalid value encountered in divide\n",
            "  image = image/(np.max(image)/255)\n",
            "<ipython-input-9-e70c224ca631>:261: RuntimeWarning: invalid value encountered in cast\n",
            "  return np.array(image, np.uint8)\n",
            "<ipython-input-9-e70c224ca631>:260: RuntimeWarning: invalid value encountered in divide\n",
            "  image = image/(np.max(image)/255)\n",
            "<ipython-input-9-e70c224ca631>:261: RuntimeWarning: invalid value encountered in cast\n",
            "  return np.array(image, np.uint8)\n",
            "<ipython-input-9-e70c224ca631>:260: RuntimeWarning: invalid value encountered in divide\n",
            "  image = image/(np.max(image)/255)\n",
            "<ipython-input-9-e70c224ca631>:261: RuntimeWarning: invalid value encountered in cast\n",
            "  return np.array(image, np.uint8)\n",
            "<ipython-input-9-e70c224ca631>:260: RuntimeWarning: invalid value encountered in divide\n",
            "  image = image/(np.max(image)/255)\n",
            "<ipython-input-9-e70c224ca631>:261: RuntimeWarning: invalid value encountered in cast\n",
            "  return np.array(image, np.uint8)\n",
            "<ipython-input-9-e70c224ca631>:260: RuntimeWarning: invalid value encountered in divide\n",
            "  image = image/(np.max(image)/255)\n",
            "<ipython-input-9-e70c224ca631>:261: RuntimeWarning: invalid value encountered in cast\n",
            "  return np.array(image, np.uint8)\n",
            "<ipython-input-9-e70c224ca631>:260: RuntimeWarning: invalid value encountered in divide\n",
            "  image = image/(np.max(image)/255)\n",
            "<ipython-input-9-e70c224ca631>:261: RuntimeWarning: invalid value encountered in cast\n",
            "  return np.array(image, np.uint8)\n",
            "<ipython-input-9-e70c224ca631>:260: RuntimeWarning: invalid value encountered in divide\n",
            "  image = image/(np.max(image)/255)\n",
            "<ipython-input-9-e70c224ca631>:261: RuntimeWarning: invalid value encountered in cast\n",
            "  return np.array(image, np.uint8)\n",
            "<ipython-input-9-e70c224ca631>:260: RuntimeWarning: invalid value encountered in divide\n",
            "  image = image/(np.max(image)/255)\n",
            "<ipython-input-9-e70c224ca631>:261: RuntimeWarning: invalid value encountered in cast\n",
            "  return np.array(image, np.uint8)\n",
            "<ipython-input-9-e70c224ca631>:260: RuntimeWarning: invalid value encountered in divide\n",
            "  image = image/(np.max(image)/255)\n",
            "<ipython-input-9-e70c224ca631>:261: RuntimeWarning: invalid value encountered in cast\n",
            "  return np.array(image, np.uint8)\n",
            "<ipython-input-9-e70c224ca631>:260: RuntimeWarning: invalid value encountered in divide\n",
            "  image = image/(np.max(image)/255)\n",
            "<ipython-input-9-e70c224ca631>:261: RuntimeWarning: invalid value encountered in cast\n",
            "  return np.array(image, np.uint8)\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "'''\n",
        "Adversarial Attack on Segmentation Models (UResNets)\n",
        "\n",
        "This program is intended to be a general tool for training any segmentation\n",
        "model and test its robustness with any attack strategy. To approach other\n",
        "models just changes the 'model.py' file, or load the desired model in the va-\n",
        "riable 'model'. And to change the attack, just change the attack class 'atk'\n",
        "with the desired attack strategy (the hint is to use Cleverhans or Torchattacks\n",
        "libraries).\n",
        "\n",
        "This algorithm uses Torchattacks library to attack a pre-trained model of 97%\n",
        "accuracy in the segmentation of cytoplasm and nuclei regions in white-blood\n",
        "cells (WBC) from blood stained slides. Using the Torchattacks library, a\n",
        "diverse range of attacks can be conducted using this same algorithm, only by\n",
        "changing the class called at the 'atk' object to the desired attack method.\n",
        "See Torchattacks documentation for more information.\n",
        "\n",
        "This program can only run with other three python files (utils.py, model.py and\n",
        "dataset.py), and with a trained model, which can be trained using the train.py\n",
        "file. It is interesting to train the model with the same dataset used in the\n",
        "attacks. Thus, specify the path for this dataset in the list variables in this\n",
        "algorithm, e.g. 'train_image_dir' (this is the same name used in train.py for\n",
        "training). All python files are available on GitHub repository (link below).\n",
        "\n",
        "- 'utils.py': used to define util functions both for attacking and training.\n",
        "- 'dataset.py': to load images in the DataLoader class of Torch.\n",
        "- 'model.py': defined the model itself (it can be changed to any desired model)\n",
        "which in this case can be either ResNets with 18, 34, 50, 101, and 152 layers.\n",
        "- 'train.py': algorithm that can be used for training any model described in\n",
        "'model.py'\n",
        "\n",
        "To attack your mode in the white-blood cells problem, you need to train it on\n",
        "WBC datasets, like the Raabin-WBC Dataset (used here), using the 'train.py'\n",
        "file, or on other dataset of your choise.\n",
        "\n",
        "P.S.: the cell with main function has to be executed separately in the python\n",
        "console (copy and paste this cell in the console separately)\n",
        "\n",
        "Find more on the GitHub Repository:\n",
        "https://github.com/MarlonGarcia/attacking-white-blood-cells\n",
        "\n",
        "@author: Marlon Rodrigues Garcia\n",
        "@instit: University of São Paulo\n",
        "'''\n",
        "\n",
        "### Program Header\n",
        "\n",
        "# If running on Colabs, mounting drive\n",
        "run_on_colabs = True\n",
        "\n",
        "# if run_on_colabs:\n",
        "#     # importing Drive\n",
        "#     from google.colab import drive\n",
        "#     drive.mount('/content/gdrive')\n",
        "#     # to import add current folder to path (import py files):\n",
        "#     import sys\n",
        "#     root_folder = r'/content/gdrive/MyDrive/...'\n",
        "#     root_model = r'/content/gdrive/MyDrive/...'\n",
        "#     sys.path.append(root_folder)\n",
        "# else:\n",
        "#     root_folder = r'C:\\Users\\marlo\\Downloads\\Dresden Dataset\\Enviar ao Thales'\n",
        "#     root_model = r'C:\\Users\\marlo\\Downloads\\Dresden Dataset\\Enviar ao Thales'\n",
        "\n",
        "if run_on_colabs:\n",
        "    # importing Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    # to import add current folder to path (import py files):\n",
        "    import sys\n",
        "    root_folder = r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Desenvolvido pelos Pesquisadores/Thales Pimentel Zuanazzi/Segmentação/teste_optimal_cross'\n",
        "    root_model  = r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Desenvolvido pelos Pesquisadores/Thales Pimentel Zuanazzi/Segmentação/teste_optimal_cross'\n",
        "    chekpoint_dir = 'my_checkpoint25.pth.tar'\n",
        "    # root_folder = r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Thales Pimentel Zuanazzi/Segmentação/training_liver'\n",
        "    # root_model  = r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Thales Pimentel Zuanazzi/Segmentação/training_liver'\n",
        "    # chekpoint_dir = 'my_checkpoint10.pth.tar'\n",
        "    # chekpoint_dir = '/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Thales Pimentel Zuanazzi/Segmentação/primeira rodada de treinamentos/training_pancreas/my_checkpoint67.pth.tar'\n",
        "    sys.path.append(root_folder)\n",
        "else:\n",
        "    root_folder = r'C:/Users/marlo/My Drive/College/Biophotonics Lab/Research/Programs/Python/Camera & Image/Surgery RGB/25.09.2023 - Algoritmo Thales'\n",
        "    root_model = r'C:/Users/marlo/My Drive/College/Biophotonics Lab/Research/Programs/Python/Camera & Image/Surgery RGB/25.09.2023 - Algoritmo Thales'\n",
        "    # chekpoint_dir = 'my_checkpoint18.pth.tar'\n",
        "\n",
        "# importing Libraries\n",
        "import os\n",
        "os.chdir(root_folder)\n",
        "# from utils import *\n",
        "# from model import *\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms.functional as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "# from cleverhans.torch.attacks.projected_gradient_descent import (\n",
        "#     projected_gradient_descent,\n",
        "# )\n",
        "\n",
        "#%% Defining Hyperparameters and Directories\n",
        "\n",
        "# hyperparameters\n",
        "epsilons = [0]\n",
        "numb_steps = 40         # number of steps for attack\n",
        "step_size = 2/255       # step size for attack\n",
        "norm = 'inf'            # norm of attack\n",
        "output_labels = 2       # number of output labels/classes\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "learning_rate = 5e-4    # learning rate\n",
        "batch_size = 2          # batch size\n",
        "num_workers = 0         # number of workers (smaller or = n° processing units)\n",
        "clip_train = 1.00       # percentage to clip the train dataset (for tests)\n",
        "clip_valid = 1.00       # percentage to clip the valid dataset (for tests)\n",
        "clip_test = 1.00        # percentage to clip the valid dataset (for tests)\n",
        "valid_percent = 0.15    # use a percent of train dataset as validation dataset\n",
        "test_percent = 0.15     # a percent from training dataset (but do not excluded)\n",
        "image_height = 512      # height to crop the image\n",
        "image_width = 640       # width to crop the image\n",
        "pin_memory = True       # setting pin memory to 'True'\n",
        "load_model = True       # 'true' to load a model and test it\n",
        "save_images = True      # saving example from predicted and original\n",
        "\n",
        "# defining the paths to datasets\n",
        "if run_on_colabs:\n",
        "    train_image_dir = [\n",
        "                       r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/liver/01',\n",
        "                       r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/liver/02',\n",
        "                       r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/liver/03',\n",
        "                       r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/liver/04',\n",
        "                       r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/liver/05',\n",
        "                       r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/liver/11',\n",
        "                       r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/liver/12',\n",
        "                       r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/liver/13',\n",
        "                       r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/liver/15',\n",
        "                       r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/liver/18',\n",
        "                       r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/liver/20',\n",
        "                       r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/liver/21',\n",
        "                       r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/liver/22',\n",
        "                       r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/liver/24',\n",
        "                       r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/liver/25',\n",
        "                       r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/liver/26',\n",
        "                       r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/liver/28',\n",
        "                       r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/liver/29',\n",
        "                       r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/liver/30',\n",
        "                       r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/liver/31'\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/abdominal_wall/03',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/abdominal_wall/04',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/abdominal_wall/05',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/abdominal_wall/06',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/abdominal_wall/07',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/abdominal_wall/09',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/abdominal_wall/11',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/abdominal_wall/12',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/abdominal_wall/14',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/abdominal_wall/16',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/abdominal_wall/17',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/abdominal_wall/18',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/abdominal_wall/19',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/abdominal_wall/20',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/abdominal_wall/21',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/abdominal_wall/22',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/abdominal_wall/23',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/abdominal_wall/24',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/abdominal_wall/25',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/abdominal_wall/26'\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/pancreas/01',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/pancreas/02',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/pancreas/03',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/pancreas/04',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/pancreas/05',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/pancreas/07',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/pancreas/08',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/pancreas/11',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/pancreas/12',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/pancreas/16',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/pancreas/17',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/pancreas/18',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/pancreas/20',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/pancreas/21',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/pancreas/22',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/pancreas/24',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/pancreas/26',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/pancreas/27',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/pancreas/29',\n",
        "                      # r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/pancreas/31'\n",
        "\n",
        "                       ]\n",
        "else:\n",
        "    train_image_dir = [r'C:/Users/marlo/Downloads/Dresden Dataset/abdominal_wall/03']\n",
        "# defining validation diretory\n",
        "if run_on_colabs:\n",
        "    val_image_dir = [r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/liver/22',\n",
        "                     r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/liver/24',\n",
        "                     r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/liver/26',]\n",
        "else:\n",
        "    val_image_dir = [r'C:/Users/marlo/Downloads/Dresden Dataset/liver/03'#,\n",
        "                      # r'C:/Users/marlo/Downloads/Dresden Dataset/abdominal_wall/24',\n",
        "                      # r'C:/Users/marlo/Downloads/Dresden Dataset/abdominal_wall/26'\n",
        "                      ]\n",
        "# images in this directory will be used to save examples of the perturbed ones\n",
        "if run_on_colabs:\n",
        "    save_image_dir = [r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/liver/03']\n",
        "else:\n",
        "    save_image_dir = [r'C:/Users/marlo/Downloads/Dresden Dataset/liver/03']\n",
        "\n",
        "# # defining the paths to datasets\n",
        "# if run_on_colabs:\n",
        "#     train_image_dir = [r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/abdominal_wall/03']\n",
        "# else:\n",
        "#     train_image_dir = [r'C:/Users/marlo/Downloads/Dresden Dataset/abdominal_wall/03']\n",
        "\n",
        "# # images in this directory will be used to save examples of the perturbed ones\n",
        "# if run_on_colabs:\n",
        "#     save_image_dir = [r'/content/gdrive/Shareddrives/Lab. de Óptica Biomédica/Datasets/DSAD/abdominal_wall/24']\n",
        "# else:\n",
        "#     save_image_dir = [r'C:/Users/marlo/Downloads/Dresden Dataset/abdominal_wall/24']\n",
        "\n",
        "\n",
        "#%% Defining Model and Loading DataLoader\n",
        "\n",
        "# defining the model\n",
        "model = UResNet34(in_channels=3, num_classes=2).to(device)\n",
        "# model = UResNet50(in_channels=3, num_classes=2).to(device)\n",
        "# initializing optimizer\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "schedule = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "# defining the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# loading the Pre-trained weights for the UResNet50\n",
        "if load_model:\n",
        "    # loading checkpoint, if 'cpu', we need to pass 'map_location'\n",
        "    os.chdir(root_model)\n",
        "    if device == 'cuda':\n",
        "        load_checkpoint(torch.load(chekpoint_dir), model)\n",
        "    else:\n",
        "        load_checkpoint(torch.load(chekpoint_dir,\n",
        "                                   map_location=torch.device('cpu')), model)\n",
        "model.to(device)\n",
        "\n",
        "#%% Running main() function, where the model will be attacked and evaluated\n",
        "\n",
        "# initiating variables of metrics to be saved\n",
        "acc_test = []\n",
        "f1_test = []\n",
        "iou_test = []\n",
        "rec_test = []\n",
        "prec_test = []\n",
        "spec_test = []\n",
        "acc_valid = []\n",
        "f1_valid = []\n",
        "iou_valid= []\n",
        "rec_valid = []\n",
        "prec_valid = []\n",
        "spec_valid = []\n",
        "\n",
        "\n",
        "def norm255(image):\n",
        "    image = image - np.min(image)\n",
        "    image = image/(np.max(image)/255)\n",
        "    return np.array(image, np.uint8)\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Here 'output_labels == 2' means binary. We use this conditional because\n",
        "    # the model we are using returns a multilabel data, when we use this model\n",
        "    # with 2 labels, it becomes binary.\n",
        "    # setting the model to evaluation\n",
        "    model.eval()\n",
        "    # creating directory to save results\n",
        "    os.chdir(root_folder)\n",
        "    # iterating in 'epsilons' to accounts for different epsilon in perturbation\n",
        "    for epsilon in epsilons:\n",
        "        ## Saving images - The next 'get_loaders' and the next 'for' loop in\n",
        "        #'save_loader' are used to save original and perturbed images. This\n",
        "        # images cannot be saved in the above loops, because the 'get_loaders'\n",
        "        # lose part of the image informatino due to its normalization\n",
        "        print('\\n\\n- Saving Images...\\n')\n",
        "        # Loading the DataLoader\n",
        "        # loading DataLoaders\n",
        "        _, _, save_loader = get_loaders(\n",
        "            train_image_dir=train_image_dir,\n",
        "            valid_percent=0,\n",
        "            test_percent=0,\n",
        "            batch_size=1,\n",
        "            image_height=image_height,\n",
        "            image_width=image_width,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=pin_memory,\n",
        "            val_image_dir=save_image_dir,\n",
        "            clip_valid=1.0,\n",
        "            clip_train=1.0,\n",
        "            clip_test=1.0\n",
        "        )\n",
        "\n",
        "        # creating directory to save results\n",
        "        os.chdir(root_folder)\n",
        "        try: os.mkdir('saved_images_attack')\n",
        "        except: pass\n",
        "        # Defining names of images to be saved (important to save with the\n",
        "        # original name for further comparison)\n",
        "        names = [name for name in os.listdir(save_image_dir[0]) if 'image' in name]\n",
        "        # Sort the image and label names based on the numeric part extracted from filenames\n",
        "        names.sort(key=lambda x: int(x[5:7]))  # Extract the two-digit number from \"imageXX.png\"\n",
        "        print(names)\n",
        "\n",
        "        masks = [name for name in os.listdir(save_image_dir[0]) if 'mask' in name]\n",
        "        masks.sort(key=lambda x: int(x[4:6]))  # Extract the two-digit number from \"imageXX.png\"\n",
        "        print(masks)\n",
        "        # using 'tqdm' library to see a progress bar\n",
        "        loop = tqdm(save_loader)\n",
        "        for i, dictionary in enumerate(loop):\n",
        "            image, label = dictionary\n",
        "            # Extracting the data from dictionary with keys 'image' and 'label'\n",
        "            x, y = dictionary[image], dictionary[label]\n",
        "            # Casting to device\n",
        "            x, y = x.to(device=device), y.to(device=device)\n",
        "            # Label needs to be float\n",
        "            y = y.float()\n",
        "            # changing from one-hot encoding to label (model output is one-hot)\n",
        "            if output_labels == 2:\n",
        "                label = y[:,0]\n",
        "            elif output_labels>2:\n",
        "                raise NotImplementedError\n",
        "            else:\n",
        "                raise ValueError('\\n\\noutput_labels has to be 2 or greater')\n",
        "            # Forward-passing the image\n",
        "            pred = model(x)\n",
        "            # changing from one-hot encoding to label\n",
        "            if output_labels == 2:\n",
        "                pred = pred[:,0]\n",
        "                # Cropping the label, in case convolutions changed its size\n",
        "                label = tf.center_crop(label, pred.shape[1:])\n",
        "            elif output_labels>2:\n",
        "                label = tf.center_crop(label, pred.shape[2:])\n",
        "                raise NotImplementedError\n",
        "            else:\n",
        "                raise ValueError('\\n\\noutput_labels has to be 2 or greater')\n",
        "            # Turning probabilities in 'True' or 'False', then in float values\n",
        "            pred = (pred > 0.5).float()\n",
        "            ## Next lines are to actually save the images\n",
        "            os.chdir(root_folder)\n",
        "            # The tensor needs to be converted to numpy to be saved\n",
        "            temp = label.to('cpu',torch.int32).numpy()[0,:,:]\n",
        "            # Function to de-normalize the tensor to the [0,255] range\n",
        "            temp = norm255(temp)\n",
        "            # Saving image with OpenCV\n",
        "            # cv2.imwrite(f'saved_images_attack/Imagem_{str(i)}_ground_truth.png', temp)\n",
        "            # Converting tensor to numpy in appropriate form\n",
        "            temp = pred.to('cpu',torch.int32).numpy()[0,:,:]\n",
        "            # De-normalizing the values\n",
        "            temp = norm255(temp)\n",
        "            # Saving image with OpenCV\n",
        "            cv2.imwrite(f'saved_images_attack/Imagem_{str(i)}_predicao.png', temp)\n",
        "            ## The next steps are to load (imread) and save (imwrite) the ori-\n",
        "            # ginal and ground truth images\n",
        "            temp = cv2.imread(save_image_dir[0]+'/'+names[i])\n",
        "            # Next steps are to return the numpy to the tensor format\n",
        "            temp = temp.astype(float)/np.max(temp)\n",
        "            temp = torch.Tensor(temp).permute(2,0,1)\n",
        "            temp = temp[None,:,:,:]\n",
        "            temp = tf.resize(temp,(image_height,image_width))\n",
        "            # Returning to the numpy format to be saved as an image\n",
        "            temp = temp.permute(0, 2, 3, 1).to('cpu').numpy()[0,:,:,:]\n",
        "            # De-normalization to the image range\n",
        "            temp = norm255(temp)\n",
        "            # Saving image\n",
        "            cv2.imwrite(f'saved_images_attack/Imagem_{str(i)}_x_original.png', temp)\n",
        "\n",
        "            temp = cv2.imread(save_image_dir[0]+'/'+masks[i])\n",
        "            # Next steps are to return the numpy to the tensor format\n",
        "            temp = temp.astype(float)/np.max(temp)\n",
        "            temp = torch.Tensor(temp).permute(2,0,1)\n",
        "            temp = temp[None,:,:,:]\n",
        "            temp = tf.resize(temp,(image_height,image_width))\n",
        "            # Returning to the numpy format to be saved as an image\n",
        "            temp = temp.permute(0, 2, 3, 1).to('cpu').numpy()[0,:,:,:]\n",
        "            # De-normalization to the image range\n",
        "            temp = norm255(temp)\n",
        "            # Saving image\n",
        "            cv2.imwrite(f'saved_images_attack/Imagem_{str(i)}_ground_truth.png', temp)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhRvIvMrK8r4PZ2aK2QnO7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ce0ae23278d84246a4daaaabdd79b449": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5043572aea3045dfa974184c5db49ced",
              "IPY_MODEL_5bcdf60e997f4fb6b4da46f604f52f7d",
              "IPY_MODEL_ca141c47bd8845669eeca932b0545d3e"
            ],
            "layout": "IPY_MODEL_e80fea8db4814a33803f07498c8de418"
          }
        },
        "5043572aea3045dfa974184c5db49ced": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d2f6d2b3c1341b897c24495dcf3a464",
            "placeholder": "​",
            "style": "IPY_MODEL_23279135b6b64fbe870e8439ccbfb1ba",
            "value": "100%"
          }
        },
        "5bcdf60e997f4fb6b4da46f604f52f7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6087cc9c09cc4e01ba87068b417ea239",
            "max": 90,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_56179e4fb7b942ea9ad9b1fab51a7df5",
            "value": 90
          }
        },
        "ca141c47bd8845669eeca932b0545d3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_355f8a1096a346e183780f42356dd100",
            "placeholder": "​",
            "style": "IPY_MODEL_45d28caccff344a1899de88a79e130cd",
            "value": " 90/90 [04:13&lt;00:00,  2.68s/it]"
          }
        },
        "e80fea8db4814a33803f07498c8de418": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d2f6d2b3c1341b897c24495dcf3a464": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23279135b6b64fbe870e8439ccbfb1ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6087cc9c09cc4e01ba87068b417ea239": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56179e4fb7b942ea9ad9b1fab51a7df5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "355f8a1096a346e183780f42356dd100": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45d28caccff344a1899de88a79e130cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
